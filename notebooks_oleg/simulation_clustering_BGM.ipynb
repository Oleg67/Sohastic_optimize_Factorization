{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ThoroughBet Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name multiarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e7b92f87a87f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYEAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrayview\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrayView\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeseriesView\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oleg/anaconda2/lib/python2.7/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_newdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     __all__ = ['add_newdocs',\n\u001b[1;32m    148\u001b[0m                \u001b[0;34m'ModuleDeprecationWarning'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oleg/anaconda2/lib/python2.7/site-packages/numpy/add_newdocs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_newdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oleg/anaconda2/lib/python2.7/site-packages/numpy/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtype_check\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindex_tricks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunction_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oleg/anaconda2/lib/python2.7/site-packages/numpy/lib/type_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m            'common_type']\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumeric\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumeric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misnan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mobj2sctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oleg/anaconda2/lib/python2.7/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menvkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0menv_added\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menvkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_added\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menvkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name multiarray"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from utils import settings, timestamp, YEAR\n",
    "from utils.arrayview import ArrayView, TimeseriesView\n",
    "\n",
    "\n",
    "from prediction.models.preprocessing import Model\n",
    "from prediction.models.prediction import factornames_trimmed\n",
    "from prediction.models.parameters import factor_build_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av = ArrayView.from_file(settings.paths.join('brain_final2cut.av.bcolz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsav = {}\n",
    "sl = 0\n",
    "while True:\n",
    "    try:\n",
    "        tsav[sl] = ArrayView.from_file(settings.paths.join('brain_final2_slice_%s.av.bcolz' % sl))\n",
    "    except ValueError:\n",
    "        break\n",
    "    sl += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mod = Model(av, oos_start=factor_build_end+YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_kurtosis_factors =  ['z64f5be67e', 'z90adc182a', 'z7081bf371', 'z34b808e99', 'z757be272e', 'z5a85cd6a9',\n",
    "                         'zf991b634a', 'z62651f605', 'zd002b7067', 'z2ef7fedca', 'z6f11029f7', 'z412893062',\n",
    "                          'z919b9585a', 'z89b0eda37', 'z31780b3f4', 'z6631693d3', 'z0b27f29ad', 'zd7cd94e4c', \n",
    "                          'zf5b2aef2a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "price_factors = ['zb392bb74a', 'z6809c316d', 'zd678f0538', 'z027f9f0f5', 'z88e79930c', 'z4a72dc02f',\n",
    "                 'z1a3573928', 'z7b15df227']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time factors = mod._preprocess_factors(factornames_trimmed, high_kurtosis_factors = high_kurtosis_factors,\\\n",
    "                                        price_factors = price_factors, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mask = mod.is1|mod.is2|mod.oos\n",
    "train_mask = mod.is1|mod.is2\n",
    "train_event_id = av.event_id[train_mask]\n",
    "predict_event_id = av.event_id[predict_mask]\n",
    "len(predict_mask), len(train_mask), train_mask.sum(), predict_mask.sum(), len(train_event_id), len(np.unique(train_event_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['f{}'.format(i) for i in range(1,58)]\n",
    "df = pd.DataFrame(data =factors[:, predict_mask].T , columns = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['event_id'] = av.event_id[predict_mask]\n",
    "df['runner_id'] = av.runner_id[predict_mask]\n",
    "df['result'] = av.result[predict_mask]\n",
    "df['is1'] = mod.is1[predict_mask]\n",
    "df['is2'] = mod.is2[predict_mask]\n",
    "df['oos'] = mod.oos[predict_mask]\n",
    "df['time'] =av.start_time[predict_mask]\n",
    "df['obstacle'] = av.obstacle[predict_mask]\n",
    "df['going'] = av.going[predict_mask]\n",
    "df['speed'] = av.speed[predict_mask]\n",
    "df['distance'] = av.distance[predict_mask]\n",
    "df['prize'] = av.prize[predict_mask]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.to_csv('/home/oleg/thbmodel/racehorse_data2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- групировка по забегам \n",
    "- df_f сгрупированы факторы по забегу c вычислением минимальной разницы между сортированными факторами для значение в забеге\n",
    "- df1 сгрупированы  переменные которые общие для всех участников звбега "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_event = lambda x: (x.max()-x.min())/x.count()\n",
    "df_f = df.ix[:,u'f1'].groupby(df['event_id']).apply(fun_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in df.ix[:,u'f2':u'f57'].columns:\n",
    "    df_f = pd.concat([df_f,df.ix[:, f].groupby(df['event_id']).apply(fun_event)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.ix[:,u'is2':].groupby(df['event_id']).first()\n",
    "df1 = df1.join(df['result'].groupby(df['event_id']).count())\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f.shape, df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### кластеризуем алгоритмом BayesianGaussianMixture¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "cl_algor = []\n",
    "score_list =[]\n",
    "for eps in [None, 5e-3, 3e-2, 0.1, 0.2, 0.4, 1.0, 10., 1e+2, 1e+3, 1e+5]:\n",
    "    if eps is None:\n",
    "        BGM = BayesianGaussianMixture(n_components=12 )\n",
    "        #BGM = BayesianGaussianMixture(n_components=12 , weight_concentration_prior_type ='dirichlet_distribution')\n",
    "        eps = 'def'\n",
    "    else:\n",
    "        BGM = BayesianGaussianMixture(n_components=12, weight_concentration_prior= eps)\n",
    "        #BGM = BayesianGaussianMixture(n_components=12, weight_concentration_prior_type ='dirichlet_distribution', \n",
    "                                      #weight_concentration_prior= eps) \n",
    "                                      \n",
    "    BGM.fit(df_f.ix[df1.is2,u'f1':u'f57'].values)\n",
    "    \n",
    "    al_name = 'BGM_p_{}'.format(eps)\n",
    "    \n",
    "    df1[al_name] = BGM.predict(df_f.ix[:,u'f1':u'f57'].values)\n",
    "    score = BGM.score(df_f.ix[:,u'f1':u'f57'].values)\n",
    "    cl_algor.append(al_name)\n",
    "    score_list.append((score, al_name))\n",
    "    print 'score  ',score\n",
    "    \n",
    "    print df1[al_name].value_counts()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- отсортируем в порядке возростания likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_algor = [x[1] for x in sorted(score_list, key = lambda x: x[0])]\n",
    "cl_algor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_algor = cl_algor[:2] + cl_algor[-2:] # 4 with max and min score\n",
    "cl_algor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- переименуем кластеры по порядку убывания в них точек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for al in cl_algor:\n",
    "    dic = {x:y for y,x in zip(['a', 'b', 'c', 'd', 'e', 'f','g','h', 'j', 'q', 'p', 's'],df1[al].value_counts().index)}\n",
    "    df1[al] = df1[al].replace(dic)\n",
    "    df_f[al] = df1[al]\n",
    "    print df1[al].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 400\n",
    "df_f.to_csv(settings.paths.join('clusterin_data_BGM_p.csv'))\n",
    "#with open (settings.paths.join('clusterin_data_kMM.pkl'.format(al)), 'wb') as data:\n",
    "            #pickle.dump( df_f, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- сравним разные алгоритмы на совпадения кластеров "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "simmilar_algorithm = []\n",
    "for (ag1, ag2) in combinations(cl_algor, 2):\n",
    "    numbe_simmilar = (df1[ag1] == df1[ag2]).sum()\n",
    "    print 'algorithms {}  = {}  in  {} case  from  {}'.format(ag1,ag2, numbe_simmilar, len(df1[ag1]))\n",
    "    print \n",
    "    if numbe_simmilar > 14000:\n",
    "        simmilar_algorithm.append((ag1,ag2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simmilar_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from prediction.models import clmodel\n",
    "from prediction.tools.helpers import strata_scale_down\n",
    "\n",
    "is1 = mod.is1.copy()\n",
    "is2 = mod.is2.copy()\n",
    "oos = mod.oos.copy()\n",
    "strata = strata_scale_down(av.event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_sets_end (data, cl, time = 'time', end = (0.75, 0.85), threshold = 400):\n",
    "    \"\"\" \n",
    "    определяем время окончания общее для всех кластеров из data[cl] так чтобы доля точек с временем меньше \n",
    "    было в интервале 'end' если не удается найти приемлемое пресечение временных интервалов выдает сообщение\n",
    "    \n",
    "    <data> - pandas dataFrame\n",
    "    <cl>  - column with names of clusters for each event_id\n",
    "    <time> - column with the start's time  for each event_id \n",
    "    <end> - part of poins that before interval \n",
    "    <threshold> minimum number of points in the cluster if the cluster is counted\n",
    "    \"\"\"\n",
    "    \n",
    "    time_end_min =[]\n",
    "    time_end_max =[]\n",
    "    df_cl = data[cl].value_counts()\n",
    "    \n",
    "    for cluster  in df_cl.index[df_cl > threshold]:\n",
    "        time_list = data[time][data[cl] == cluster].tolist()\n",
    "        time_end_min.append(time_list[int(len(time_list)*end[0])]) # время  окончания трайн min\n",
    "        time_end_max.append(time_list[int(len(time_list)*end[1])]) # время окончания трайн max\n",
    "        \n",
    "    time_min = np.max(time_end_min)\n",
    "    time_max = np.min(time_end_max)\n",
    "    \n",
    "    if time_min <= time_max:\n",
    "        return (time_min+time_max)/2.\n",
    "    else:\n",
    "        print 'not itersection of time sets for {}  {}'.format( cl, end)\n",
    "        return time_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(data, cl, time = 'time', train = (0.75, 0.85), val = (0.5, 0.65), threshold = 400, verbose=False):\n",
    "    \"\"\" \n",
    "    разбивает выборку на train, validation, test так чтобы доля точек в train set была в пределах 'train'\n",
    "    а оставшиеся точки разбиваются на validation & test в соотношении 'val',\n",
    "    при этом время всех точек по наростающей\n",
    "    <data> - pandas dataFrame\n",
    "    <cl>  - column with names of clusters for each event_id\n",
    "    <time> - column with the start's time  for each event_id \n",
    "    <train> - part of points  for train\n",
    "    <val> -  part of points  for validation from (all set - train)\n",
    "    <threshold> minimum number of points in the cluster if the cluster is counte\n",
    "    \"\"\"\n",
    "    \n",
    "    train_event, val_event, test_event =[], [],[]\n",
    "    \n",
    "    time_train_end = time_sets_end( data, cl , end =train, threshold = threshold) # время  окончания трайн\n",
    "    train_event = data.index[data[time] <= time_train_end].tolist() \n",
    "    \n",
    "    df_val = data[data[time] > time_train_end]\n",
    "    time_test_start = time_sets_end(df_val , cl, end =val) # время  окончания validation\n",
    "    \n",
    "    if verbose:\n",
    "        print 'time train end', time_train_end\n",
    "        print 'time test start', time_test_start\n",
    "    val_event = data.index[(data['time'] >time_train_end) & (data['time'] <= time_test_start)].tolist()\n",
    "    test_event = data.index[data['time'] > time_test_start].tolist()\n",
    "    \n",
    "    return (train_event, val_event, test_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts_mask = av.start_time >= float(timestamp('2015-08-01'))\n",
    "\n",
    "for k in tsav.keys():\n",
    "    tsav[k] = tsav[k][predict_mask [ts_mask]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step2a(data, cluster_list, is1, oos, av =av, tsav = tsav, factors = factors, mod = mod, verbose=False):\n",
    "    \n",
    "    model_coefs, model_step1prob, model_step2prob, model_likelihood = {}, {}, {}, {}\n",
    "    \n",
    "    \n",
    "    df_cl = data.value_counts()\n",
    "\n",
    "    for cluster in cluster_list:\n",
    "\n",
    "        mask_cluster = np.in1d(av.event_id, data.index[data == cluster])\n",
    "        \n",
    "        mod.is1 = is1 & mask_cluster\n",
    "        mod.is2 = is1 & mask_cluster\n",
    "        mod.oos = oos & mask_cluster\n",
    "\n",
    "        \n",
    "         \n",
    "        model_coefs[cluster], model_step1prob[cluster], model_step2prob[cluster], model_likelihood[cluster]\\\n",
    "        = mod.fit_slices(tsav, factors,  depth=3, lmbd=10, verbose=False, fit_afresh=True)\n",
    "        if verbose:\n",
    "            print 'cluster {}  number  {}'.format(cluster, df_cl[cluster])\n",
    "            train_event = np.unique(av.event_id[mod.is1])\n",
    "            test_event = np.unique(av.event_id[mod.oos])\n",
    "            print 'LL  {}          {}            {}'.format (len(train_event), len(train_event), len(test_event))\n",
    "            print model_likelihood[cluster]\n",
    "    return model_coefs, model_step1prob, model_step2prob, model_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_lists(data, oos, threshold, min_test =5, verbose=False):\n",
    "    \"\"\"\n",
    "    return\n",
    "    cluster_list the list of cluster where each cluster has test points more that min_test \n",
    "    \n",
    "    <data>  pandas Series with index are event_id , data are clusters names\n",
    "    <oos>   test mask\n",
    "    <threshold> minimum number of points in the cluster if the cluster is counte\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df_cl = data.value_counts()\n",
    "    cluster_list = []\n",
    "   \n",
    "    \n",
    "    for cluster  in df_cl.index[df_cl > threshold]:\n",
    "        \n",
    "        cluster_mask = np.in1d(av.event_id, data.index[data == cluster])\n",
    "        oos_event = np.unique(av.event_id[oos & cluster_mask])\n",
    "        \n",
    "        if len(oos_event) >= min_test:\n",
    "            \n",
    "            cluster_list.append(cluster)\n",
    "            \n",
    "    return cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_model_coefs, cluster_model_step1prob, cluster_model_step2prob, cluster_model_likelihood = {}, {}, {}, {}\n",
    "#old_model_coefs, old_model_step1prob, old_model_step2prob, old_model_likelihood= {}, {}, {}, {}\n",
    "\n",
    "cluster_list ={}\n",
    "\n",
    "for al in cl_algor:\n",
    "    print al\n",
    "    \n",
    "    cluster_list[al] = cluster_lists(df1[al], oos, threshold = threshold)\n",
    "    \n",
    "    #cluster_model_coefs[al], cluster_model_step1prob[al],  cluster_model_step2prob[al],\\\n",
    "    #cluster_model_likelihood[al]= step2 (df1[al], train_val_test[al], verbose =True)\n",
    "    cluster_model_coefs, cluster_model_step1prob, cluster_model_step2prob, cluster_model_likelihood \\\n",
    "    =    step2a (df1[al], cluster_list[al], is1, oos, verbose =True)\n",
    "    \n",
    "    print \n",
    "    \"\"\"\n",
    "    with open (settings.paths.join('clusterin_{}_cofs.pkl'.format(al)), 'wb') as f_cof:\n",
    "            pickle.dump( cluster_model_coefs, f_cof)\n",
    "            \n",
    "    with open (settings.paths.join('clusterin_{}_step1prob.pkl'.format(al)), 'wb') as f_1prob:\n",
    "            pickle.dump( cluster_model_step1prob, f_1prob)\n",
    "            \n",
    "    with open (settings.paths.join('clusterin_{}_step2prob.pkl'.format(al)), 'wb') as f_2prob:\n",
    "            pickle.dump( cluster_model_step2prob, f_2prob)\n",
    "    \"\"\"      \n",
    "mod.is1 = is1\n",
    "mod.is2 = is1\n",
    "mod.oos = oos\n",
    "    \n",
    "#old_model_coefs[al], old_model_step1prob[al], old_model_step2prob[al],\\\n",
    "#old_model_likelihood[al] = mod.fit_slices(tsav, factors, depth=3, lmbd=10, verbose=False, fit_afresh=True)\n",
    "old_model_coefs, old_model_step1prob, old_model_step2prob, old_model_likelihood \\\n",
    "    =   mod.fit_slices(tsav, factors, depth=3, lmbd=10, verbose=False, fit_afresh=True)\n",
    "    \n",
    "\n",
    "print 'No clustering '\n",
    "print 'LL'\n",
    "print old_model_likelihood\n",
    "\"\"\"\n",
    "with open (settings.paths.join('old_model_cofs.pkl'), 'wb') as f_cof:\n",
    "    pickle.dump( old_model_coefs, f_cof)\n",
    "            \n",
    "with open (settings.paths.join('old_model_step1prob.pkl'), 'wb') as f_1prob:\n",
    "    pickle.dump( old_model_step1prob, f_1prob)\n",
    "            \n",
    "with open (settings.paths.join('old_model_step2prob.pkl'), 'wb') as f_2prob:\n",
    "    pickle.dump( old_model_step2prob, f_2prob)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f.ix[:,'f1':'f57'].groupby(df_f['BGM_p_def']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "col = 'BGM_p_def'\n",
    "df__f = df_f[(df_f[col]== 'a') |(df_f[col] =='b') |(df_f[col] =='c') |(df_f[col] =='d')]\n",
    "df__f['f25'].hist( bins = 60, by=df__f[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df1['BGM_p_def'] ), old_model_step2prob.shape, cluster_model_step2prob.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.accumarray import uaccum\n",
    "from prediction.tools.helpers import strata_scale_down\n",
    "strata = strata_scale_down(av.event_id[predict_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll_diff (prob_new, prob_old, is1= is1, oos =oos, \n",
    "             av =av, tsav =tsav, strata = strata):\n",
    "    \"\"\" \n",
    "    count the differance of Likelihood for two models \n",
    "    <prob_new>  probability of new model\n",
    "    <prob_old>  probability of old model\n",
    "    <train, val, test> are lists of events for train, validation, test\n",
    "    \"\"\"\n",
    "    \n",
    "    llcomb = np.zeros((11, 3))\n",
    "    ll_old = np.zeros((11, 3))\n",
    "        \n",
    "    is2 = is1\n",
    "    predict_mask = is1|is2|oos\n",
    "            \n",
    "    for sl in xrange(10):\n",
    "        good = ~np.isnan(tsav[sl + 1].log_pmkt_back) & ~np.isnan(tsav[sl + 1].log_pmkt_lay)\n",
    "        good = uaccum(strata, good, func='all')\n",
    "        for i, mask in enumerate([is1, is2, oos]):\n",
    "            p_new = prob_new[sl, mask[predict_mask] & good] # probobility new model\n",
    "            p_old = prob_old[sl, mask[predict_mask] & good] # probobility old model\n",
    "            \n",
    "            winners = av.result[predict_mask][mask[predict_mask] & good] == 1\n",
    "\n",
    "            llcomb[sl, i] = np.mean(np.log(p_new[winners][p_new[winners] !=0])) * 1000 # LL new model\n",
    "            ll_old[sl, i] = np.mean(np.log(p_old[winners][p_old[winners] !=0])) * 1000 # LL old model\n",
    "        #print llcomb[sl, i] - ll_old[sl, i]\n",
    "            \n",
    "    diff_new_old = llcomb- ll_old # differance between mix  and old model \n",
    "    return diff_new_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll_for_each_cluster (data,  new_Model, old_Model, is1 =is1, oos =oos, best ='train', not_list =np.array([]), \n",
    "                         av =av, verbose=False):\n",
    "    \"\"\" \n",
    "    строим новую модель заменяя в старой один кластер \n",
    "    расчитываем изменеие LL в среднем по train, validation, test \n",
    "    отбираем кластеры по лучшему среднему улучшению для train or validation\n",
    "    <data> - pandas Series with index are event_id , data are clusters names \n",
    "    <new_Model>  wins probability for each cluster \n",
    "    <old_Model>  wins probability for no cluster \n",
    "    <train_val_test>  event_id for train , validation , test \n",
    "    <best>  the choose from train or validadion\n",
    "    <not_list>  the list with clusters that to exclude from model\n",
    "    \"\"\"\n",
    "    \n",
    "    if best == 'train':\n",
    "        best = 0\n",
    "    else:\n",
    "        best = 1\n",
    "    \n",
    "    \n",
    "    mean_new =[]\n",
    "    is2 = is1\n",
    "    predict_mask = is1|is2|oos\n",
    "    \n",
    "    for cluster in new_Model.keys():\n",
    "        \n",
    "        if not cluster in not_list:\n",
    "            cluster_mask = np.in1d(av.event_id[predict_mask], data.index[data == cluster])\n",
    "            prob_mix = np.where(cluster_mask , new_Model[cluster] , old_Model )\n",
    "        \n",
    "\n",
    "            diff_new_old = ll_diff(prob_mix, old_Model)\n",
    "\n",
    "            print 'cluster ', cluster\n",
    "            if verbose:\n",
    "                print diff_new_old\n",
    "    \n",
    "            mean_ll_diff = diff_new_old[:10].mean(axis =0)\n",
    "            print 'mean  ', mean_ll_diff\n",
    "            if mean_ll_diff[best] > 0:\n",
    "                mean_new.append((cluster,mean_ll_diff[best]))\n",
    "    cl_list = [x[0] for x in sorted(mean_new , key = lambda x: x[1], reverse =True)]\n",
    "    return cl_list, mean_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll_for_mix_clusters (data, cl_list, new_Model, old_Model, is1 =is1, oos =oos, best ='test', \n",
    "                         av =av, tsav =tsav, strata =strata, predict_mask =predict_mask, verbose=False):\n",
    "    \"\"\" \n",
    "    строим новую модель заменяя в старой некоторые кластеры из списка полученного от  ll_for_each_cluster()\n",
    "    расчитываем изменеие LL в среднем по train, validation, test \n",
    "    печатаем лучшее улучшение для test\n",
    "    <data> - pandas Series with index are event_id , data are clusters names\n",
    "    <cl_list> list for mix clusters\n",
    "    <new_Model>  wins probability for each cluster \n",
    "    <old_Model>  wins probability for no cluster \n",
    "    <train_val_test> event_id for train , validation , test \n",
    "    <best>  the choose from train or validadion\n",
    "    \"\"\"\n",
    "    \n",
    "    cl_lists = []\n",
    "    is2 = is1\n",
    "    predict_mask = is1|is2|oos\n",
    "    \n",
    "    for i in range(len(cl_list)):\n",
    "        cl_lists.append(cl_list[:i+1])\n",
    "        \n",
    "    if best =='test':\n",
    "        best = 2\n",
    "    elif best =='val':\n",
    "        best = 1\n",
    "    else:\n",
    "        best = 0\n",
    "\n",
    "    best_ll = 0.\n",
    "    best_mix = None\n",
    "    \n",
    "    for list_ in cl_lists:\n",
    "        print list_\n",
    "        prob_mix = old_Model\n",
    "        \n",
    "        for cluster in list_:\n",
    "\n",
    "                cluster_mask = np.in1d(av.event_id[predict_mask], data.index[data == cluster])\n",
    "                prob_mix = np.where(cluster_mask , new_Model[cluster],  prob_mix)\n",
    "                \n",
    "        diff_new_old = ll_diff(prob_mix, old_Model)\n",
    "        if verbose:\n",
    "            print '    train       validation      test'\n",
    "            print diff_new_old      # differance between mix  and old model\n",
    "        mean_ll_diff = diff_new_old[:10].mean(axis =0)\n",
    "        print 'mean ', mean_ll_diff\n",
    "        if mean_ll_diff[best] > best_ll:\n",
    "            best_mix = list_\n",
    "            best_ll = mean_ll_diff[best]\n",
    "\n",
    "\n",
    "    return best_mix, best_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- отбор лучших по трейн дата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_list = {}\n",
    "cl_means ={}\n",
    "#threshold = 100\n",
    "#cl_algor = ['BGM_0.001','BGM_0.1', 'BGM_1.0', 'kMM']\n",
    "for al in cl_algor:\n",
    "    print al\n",
    "    \n",
    "    #with open (settings.paths.join('clusterin_{}_step2prob.pkl'.format(al)), 'rb') as f_2prob:\n",
    "            #cluster_model_step2prob = pickle.load( f_2prob)\n",
    "    #with open (settings.paths.join('old_model_step2prob.pkl'), 'rb') as f_2prob:\n",
    "            #old_model_step2prob = pickle.load( f_2prob)\n",
    "    \n",
    "    cl_list[al], cl_means[al] = ll_for_each_cluster (df1[al],  cluster_model_step2prob, old_model_step2prob)\n",
    "    print cl_list\n",
    "    print \n",
    "for al in cl_algor:\n",
    "    \n",
    "    #with open (settings.paths.join('clusterin_{}_step2prob.pkl'.format(al)), 'rb') as f_2prob:\n",
    "            #cluster_model_step2prob = pickle.load( f_2prob)\n",
    "    #with open (settings.paths.join('old_model_step2prob.pkl'), 'rb') as f_2prob:\n",
    "            #old_model_step2prob = pickle.load( f_2prob)\n",
    "         \n",
    "    best_cl, best_score = ll_for_mix_clusters (df1[al], cl_list[al], cluster_model_step2prob, old_model_step2prob)\n",
    "    print \n",
    "    print 'for {}  best clusters from train score are {}  best score on test {}'.format(al, best_cl, best_score)\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### write the simdata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dic_to_tenzor(dic, key, base):\n",
    "    '''\n",
    "    tenzor where first dimention is the number of cluster\n",
    "    0 = no_cluster\n",
    "    <dic> dictionary cluster's data that to convert in tenzor\n",
    "    <key> the list of clusters that use \n",
    "    <base> no cluster data\n",
    "    '''\n",
    "    \n",
    "    key_0 = dic.keys()[0]\n",
    "    tenzor = np.zeros((len(key)+1, dic[key_0].shape[0], dic[key_0].shape[1]))\n",
    "    try:\n",
    "        tenzor[0,:,:] = base\n",
    "    except:\n",
    "        print 'base and dic[k] have the diferent size'\n",
    "        return\n",
    "    for i,k in enumerate(key):\n",
    "        tenzor[i+1,:,:] = dic[k]\n",
    "    return tenzor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clusters_number(data, key, av=av):\n",
    "    \"\"\" \n",
    "    list with numbers of clusters \n",
    "    <data>  pandas Series index = event_id, data = cluster's names\n",
    "    <key> the list of clusters that use\n",
    "    \"\"\"\n",
    "    \n",
    "    cl_number = np.zeros((len(av.event_id)))\n",
    "    for i,k in enumerate(key):\n",
    "        mask = np.in1d(av.event_id,data.index[data ==k])\n",
    "        cl_number = np.where(mask,i+1,cl_number)\n",
    "    return cl_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_simdata(step1probs, oos, coefs, cluster_number=None, file_ = 'simdata.p'):\n",
    "    '''\n",
    "    <step1probs> is expected to be a matrix N_slices x len(av). \n",
    "    <oos> is a boolean mask denoting the out of sample range. len(oos) shoud equal len(av)\n",
    "    <coefs> is a coefficient matrix with the size N_slices x 3\n",
    "    <cluster_number> is an integer array with the cluster numbers per race. Size: len(av)\n",
    "    '''\n",
    "    f = file(settings.paths.join(file_), 'wb')\n",
    "    if cluster_number is None:\n",
    "        s1p = step1probs[:, oos]\n",
    "    else:\n",
    "        cluster_number = cluster_number[oos]\n",
    "        s1p = step1probs[:, :, oos]\n",
    "    pickle.dump([s1p, oos, coefs, cluster_number], f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_dic_to_simdata(file_name, old_step1probs, old_coefs, oos, data=None, av =av,\n",
    "                         cluster_step1probs =None, cluster_coefs =None, cluster_names =None):\n",
    "    \"\"\"\n",
    "    <file_name> is name of file to record\n",
    "    <old_step1probs> is expected to be a matrix N_slices x len(av)\n",
    "    <old_coefs> is a coefficient matrix with the size N_slices x 3\n",
    "    <oos> is a boolean mask denoting the out of sample range. len(oos) shoud equal len(av)\n",
    "    <data>  pandas Series index = event_id, data = cluster's names\n",
    "    <cluster_step1probs> is expected to be a dictionary: key is the cluster name and\n",
    "                        data are the matrix N_slices x len(av) for each cluster\n",
    "    <cluster_coefs> is a dictionary : key is the cluster name and data and \n",
    "                        data are the coefficient matrix with the size N_slices x 3\n",
    "    <cluster_number> is an integer array with the cluster numbers per race. Size: len(av)\n",
    "    \"\"\"\n",
    "    \n",
    "    cl_number= np.zeros((len(av.event_id)))\n",
    "    \n",
    "    if cluster_names is not None:\n",
    "        \n",
    "        s1prob = dic_to_tenzor(cluster_step1probs, cluster_names, old_step1probs)\n",
    "        coef_s = dic_to_tenzor(cluster_coefs, cluster_names, old_coefs)\n",
    "        \n",
    "        for i,k in enumerate(cluster_names):\n",
    "            mask = np.in1d(av.event_id,data.index[data ==k])\n",
    "            cl_number = np.where(mask, i+1, cl_number)\n",
    "        #cl_number = clusters_number(data, cluster_names, av=av)\n",
    "          \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        s1prob = np.zeros((1,old_step1probs.shape[0], old_step1probs.shape[1]))\n",
    "        s1prob[0,:,:] = old_step1probs\n",
    "        coef_s = np.zeros((1,old_coefs.shape[0], old_coefs.shape[1]))\n",
    "        coef_s[0,:,:] = old_coefs\n",
    "        \n",
    "        \n",
    "\n",
    "    #write_simdata(s1prob, oos, coef_s, cl_number, file_ = file_name)\n",
    "    s1prob = s1prob[:,:,oos]\n",
    "    cl_number = cl_number[oos]\n",
    "    \n",
    "    with open (settings.paths.join(file_name), 'wb') as f:\n",
    "            pickle.dump( [s1prob, oos, coef_s, cl_number], f)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_algor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "al, cl = 'BGM_p_def' , ['h', 'b', 'q', 'a', 'c', 'e', 'f', 'd', 'j']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_write = 'simdata{}_{}.p'.format(al,''.join(cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al, 'clusterin_{}_cofs.pkl'.format(al), file_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "    \n",
    "with open (settings.paths.join('clusterin_{}_cofs.pkl'.format(al)), 'rb') as f_cof:\n",
    "    cluster_model_coefs = pickle.load( f_cof)\n",
    "            \n",
    "with open (settings.paths.join('clusterin_{}_step1prob.pkl'.format(al)), 'rb') as f_1prob:\n",
    "    cluster_model_step1prob = pickle.load(f_1prob)\n",
    "    \n",
    "#with open (settings.paths.join('old_model_cofs.pkl'.format(al)), 'rb') as f_cof:\n",
    "    #old_model_coefs = pickle.load( f_cof)\n",
    "            \n",
    "#with open (settings.paths.join('old_model_step1prob.pkl'.format(al)), 'rb') as f_1prob:\n",
    "    #old_model_step1prob = pickle.load(f_1prob)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dic_to_simdata(file_write, old_model_step1prob, old_model_coefs, oos, data=df1[al], av =av,\\\n",
    "        cluster_step1probs = cluster_model_step1prob, cluster_coefs = cluster_model_coefs, cluster_names = cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dic_to_simdata('simdata_Oldmodel.p', old_model_step1prob, old_model_coefs, oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
