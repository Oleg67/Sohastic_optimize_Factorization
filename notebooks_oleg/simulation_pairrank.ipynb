{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ThoroughBet Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>runner_id</th>\n",
       "      <th>result</th>\n",
       "      <th>event_id</th>\n",
       "      <th>is1</th>\n",
       "      <th>is2</th>\n",
       "      <th>oos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.152068</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.971937</td>\n",
       "      <td>1.096287</td>\n",
       "      <td>-0.003309</td>\n",
       "      <td>1.761018</td>\n",
       "      <td>0.968297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040241</td>\n",
       "      <td>-0.003450</td>\n",
       "      <td>-0.003327</td>\n",
       "      <td>0.036781</td>\n",
       "      <td>353046</td>\n",
       "      <td>3</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.137530</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.398019</td>\n",
       "      <td>0.481980</td>\n",
       "      <td>-0.002772</td>\n",
       "      <td>1.682896</td>\n",
       "      <td>0.354919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179168</td>\n",
       "      <td>-0.002913</td>\n",
       "      <td>-0.002791</td>\n",
       "      <td>0.213007</td>\n",
       "      <td>352217</td>\n",
       "      <td>5</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.029570</td>\n",
       "      <td>-0.145221</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.307072</td>\n",
       "      <td>0.427015</td>\n",
       "      <td>-0.012089</td>\n",
       "      <td>1.165470</td>\n",
       "      <td>0.440719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217412</td>\n",
       "      <td>-0.012249</td>\n",
       "      <td>-0.012108</td>\n",
       "      <td>-0.173009</td>\n",
       "      <td>342962</td>\n",
       "      <td>13</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.136884</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.234280</td>\n",
       "      <td>0.361007</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>0.880806</td>\n",
       "      <td>1.230350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399248</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.326784</td>\n",
       "      <td>361432</td>\n",
       "      <td>1</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.343377</td>\n",
       "      <td>0.242782</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.017472</td>\n",
       "      <td>-0.199782</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.481565</td>\n",
       "      <td>1.404777</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.325201</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>-0.145831</td>\n",
       "      <td>359420</td>\n",
       "      <td>6</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2        f3        f4      f5        f6        f7  \\\n",
       "0  0.064913  1.395654 -0.152068  0.008172  0.0007  0.971937  1.096287   \n",
       "1 -0.250759  1.137530  0.019747  0.008894  0.0007  0.398019  0.481980   \n",
       "2 -0.250759  1.029570 -0.145221 -0.020776  0.0007  0.307072  0.427015   \n",
       "3  0.064913  1.395654 -0.136884 -0.020776  0.0007  0.234280  0.361007   \n",
       "4  0.064913  1.343377  0.242782  0.008894  0.0007 -0.017472 -0.199782   \n",
       "\n",
       "         f8        f9       f10  ...         f54       f55       f56  \\\n",
       "0 -0.003309  1.761018  0.968297  ...    0.040241 -0.003450 -0.003327   \n",
       "1 -0.002772  1.682896  0.354919  ...    0.179168 -0.002913 -0.002791   \n",
       "2 -0.012089  1.165470  0.440719  ...    0.217412 -0.012249 -0.012108   \n",
       "3  0.009897  0.880806  1.230350  ...    0.399248  0.009783  0.009877   \n",
       "4  0.004808  0.481565  1.404777  ...   -0.325201  0.004683  0.004789   \n",
       "\n",
       "        f57  runner_id  result  event_id   is1   is2    oos  \n",
       "0  0.036781     353046       3    287398  True  True  False  \n",
       "1  0.213007     352217       5    287398  True  True  False  \n",
       "2 -0.173009     342962      13    287398  True  True  False  \n",
       "3  0.326784     361432       1    287398  True  True  False  \n",
       "4 -0.145831     359420       6    287398  True  True  False  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('racehorse_data.csv', index_col=[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def set_parwise(ar):\n",
    "    one = np.array(np.random.choice(ar))\n",
    "    ar_one = np.setdiff1d(ar, one)\n",
    "    one_ar = np.repeat(one, len(ar_one))\n",
    "    return [x for x in zip(one_ar, ar_one)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cut(data, result):\n",
    "    \n",
    "    event = np.intersect1d(data[data.result == result-1]['event_id'], data[data.result == result]['event_id'])\n",
    "    mask = np.in1d(data['event_id'],event)\n",
    "    return data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = df.columns[:-6].tolist()+['event_id', 'result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_diff (data, results):\n",
    "    \n",
    "    first, second = results\n",
    "    if not type(data) is np.ndarray:\n",
    "        X = data.values\n",
    "    mask1 = np.in1d(X[:,-1], first)\n",
    "    mask2 = np.in1d(X[:,-1], second)\n",
    "    X_diff = X[mask1] - X[mask2]\n",
    "    rand = np.random.choice([1,-1], len(X_diff))\n",
    "    return X_diff*rand.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- парное ранжирование для мест 1и 2, 2и 3, 3 и 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_data(df, list_result =[2, 3, 4], \n",
    "                columns = ['f{}'.format(i) for i in range(1,58)] +['result','event_id']):\n",
    "    \n",
    "    df_event = pd.Series()\n",
    "    \n",
    "    X_data = np.zeros((0,len(columns)-1))\n",
    "    for n in list_result:\n",
    "        print ''\n",
    "        df_r = df_cut(df, n) # only data where are first and second place\n",
    "        first , second = n-1, n # place 1, 2 \n",
    "        # data where first place\n",
    "        X1 = df_r[columns][df_r['result'] == first].drop_duplicates(subset ='event_id').values\n",
    "        # events which use in data\n",
    "        df_event = df_event.append(df_r['event_id'][df_r['result'] == first])\n",
    "        #print df_event.shape\n",
    "        # data where second place\n",
    "        X2 = df_r[columns][df_r['result'] == second].drop_duplicates(subset ='event_id').values\n",
    "        # differance between first and second place mix of the order  from \"first -second\" or \"second -first\"\n",
    "        X = (X1 -X2)* np.random.choice([1,-1], len(X1)).reshape(-1,1) \n",
    "        X_data = np.vstack((X_data, X[:, :-1])) # union of set of deta matrix\n",
    "        #print X.shape, X_data.shape\n",
    "        mask_class = np.in1d(X_data[:,-1],[-1.,1.]) # only two class \"-1, 1\"\n",
    "    return X_data[mask_class], df_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_llhood (data, clf, result =[2,3,4], \n",
    "               columns = ['f{}'.format(i) for i in range(1,58)] +['result','event_id']):\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    #print np.sum(data.is1), result\n",
    "    X_y_data, _ = ranking_data(data[data.is1], result, columns)\n",
    "    X_train, y_train = X_y_data[:,:-1], X_y_data[:,-1]\n",
    "    clf.fit(X_train, y_train)\n",
    "    print clf\n",
    "    X_y_data, _ = ranking_data(data[data.oos], result, columns)\n",
    "    X_test, y_test = X_y_data[:,:-1], X_y_data[:,-1]\n",
    "    print 'accuracy  test', accuracy_score ( y_test, clf.predict(X_test))\n",
    "    print\n",
    "    factors = columns[:-2]\n",
    "    try:\n",
    "        data['S_%s'%str(clf)[:6]] = clf.decision_function(data[factors].values)\n",
    "        data['p_%s'%str(clf)[:6]] = data['S_%s'%str(clf)[:6]].groupby(by = data.event_id).apply(softmax)\n",
    "    except:\n",
    "        data['p_%s'%str(clf)[:6]] =data[factors].groupby(by = data.event_id).apply(lambda x: P_win_from_pair(x, clf))\n",
    "    \n",
    "    data['log_p_%s'%str(clf)[:6]] = np.log(data['p_%s'%str(clf)[:6]])\n",
    "    print 'is1  mean ll %s'%str(clf)[:6], data.ix[data.is1 & (data.result ==1),'log_p_%s'%str(clf)[:6]].mean()*1000\n",
    "    print \n",
    "    print 'oos  mean ll %s'%str(clf)[:6], data.ix[data.oos & (data.result ==1),'log_p_%s'%str(clf)[:6]].mean()*1000\n",
    "    return data, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clustering(cluster, X_train, y_train):\n",
    "    \n",
    "    X_tr, y_tr = {}, {}\n",
    "    for cl in np.unique(cluster):\n",
    "        mask = np.in1d(cluster, cl)\n",
    "        X_tr[cl] = X_train[mask]\n",
    "        y_tr[cl] = y_train[mask]\n",
    "    return X_tr, y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_features(X, X_pca, rank=2):\n",
    "    \n",
    "    for n in range(rank):\n",
    "        X_ = X *X_pca[:,n].reshape(-1,1)\n",
    "        if n ==0:\n",
    "            X_new = np.hstack((X, X_))\n",
    "        else:\n",
    "            X_new = np.hstack((X_new, X_))\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA_features(X, pca):\n",
    "    \n",
    "    try:\n",
    "        pca.noise_variance_\n",
    "        X_pca = pca.transform(X)\n",
    "    except AttributeError:\n",
    "        X_pca = pca.fit_transform(X)\n",
    "    rank = pca.n_components_\n",
    "    X_new = new_features(X, X_pca, rank)\n",
    "    return X_new, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- доболнительные признаки add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_max_min(data):\n",
    "    \n",
    "    n, m = data.shape\n",
    "    max_min = np.zeros((n,m))\n",
    "    index  = data.index\n",
    "    #print index\n",
    "    for i,j in enumerate(index):\n",
    "        #print np.setdiff1d(index, i)\n",
    "        data_ = data.ix[np.setdiff1d(index, j), :].values\n",
    "        #print data_[:2]\n",
    "        max_min[i, :] = (data_.max(axis =0) - data_.min(axis =0))/n\n",
    "        \n",
    "    return pd.DataFrame(data = max_min, columns = data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>M_f48</th>\n",
       "      <th>M_f49</th>\n",
       "      <th>M_f50</th>\n",
       "      <th>M_f51</th>\n",
       "      <th>M_f52</th>\n",
       "      <th>M_f53</th>\n",
       "      <th>M_f54</th>\n",
       "      <th>M_f55</th>\n",
       "      <th>M_f56</th>\n",
       "      <th>M_f57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.152068</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.971937</td>\n",
       "      <td>1.096287</td>\n",
       "      <td>-0.003309</td>\n",
       "      <td>1.761018</td>\n",
       "      <td>0.968297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.092629</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.058936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.137530</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.398019</td>\n",
       "      <td>0.481980</td>\n",
       "      <td>-0.002772</td>\n",
       "      <td>1.682896</td>\n",
       "      <td>0.354919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005310</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.058936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.029570</td>\n",
       "      <td>-0.145221</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.307072</td>\n",
       "      <td>0.427015</td>\n",
       "      <td>-0.012089</td>\n",
       "      <td>1.165470</td>\n",
       "      <td>0.440719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.057598</td>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.058936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.136884</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.234280</td>\n",
       "      <td>0.361007</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>0.880806</td>\n",
       "      <td>1.230350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.042310</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.050184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.343377</td>\n",
       "      <td>0.242782</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>-0.017472</td>\n",
       "      <td>-0.199782</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.481565</td>\n",
       "      <td>1.404777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.084504</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.058936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.182570</td>\n",
       "      <td>1.029570</td>\n",
       "      <td>0.201410</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>-0.002216</td>\n",
       "      <td>0.023445</td>\n",
       "      <td>-0.225265</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.162080</td>\n",
       "      <td>0.121588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.055455</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.058936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>0.508767</td>\n",
       "      <td>-0.080697</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>-0.000843</td>\n",
       "      <td>0.130482</td>\n",
       "      <td>-0.225265</td>\n",
       "      <td>-0.002333</td>\n",
       "      <td>0.032518</td>\n",
       "      <td>-0.057125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.058936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.182570</td>\n",
       "      <td>0.336260</td>\n",
       "      <td>-0.055067</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>-0.003430</td>\n",
       "      <td>0.044895</td>\n",
       "      <td>-0.225265</td>\n",
       "      <td>-0.003962</td>\n",
       "      <td>-0.033998</td>\n",
       "      <td>0.032210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007059</td>\n",
       "      <td>0.064741</td>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.058936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.114914</td>\n",
       "      <td>0.508767</td>\n",
       "      <td>-0.100332</td>\n",
       "      <td>0.010256</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.341749</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.481565</td>\n",
       "      <td>-0.507366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.058936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.023268</td>\n",
       "      <td>0.336260</td>\n",
       "      <td>0.033493</td>\n",
       "      <td>-0.001754</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>-0.160113</td>\n",
       "      <td>-0.357380</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>-0.110449</td>\n",
       "      <td>-0.385231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.058936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2        f3        f4        f5        f6        f7  \\\n",
       "0  0.064913  1.395654 -0.152068  0.008172  0.000700  0.971937  1.096287   \n",
       "1 -0.250759  1.137530  0.019747  0.008894  0.000700  0.398019  0.481980   \n",
       "2 -0.250759  1.029570 -0.145221 -0.020776  0.000700  0.307072  0.427015   \n",
       "3  0.064913  1.395654 -0.136884 -0.020776  0.000700  0.234280  0.361007   \n",
       "4  0.064913  1.343377  0.242782  0.008894  0.000700 -0.017472 -0.199782   \n",
       "5  0.182570  1.029570  0.201410  0.003267 -0.002216  0.023445 -0.225265   \n",
       "6  0.064913  0.508767 -0.080697  0.002984 -0.000843  0.130482 -0.225265   \n",
       "7  0.182570  0.336260 -0.055067  0.001173 -0.003430  0.044895 -0.225265   \n",
       "8 -0.114914  0.508767 -0.100332  0.010256  0.000700  0.260870  0.341749   \n",
       "9 -0.023268  0.336260  0.033493 -0.001754  0.000191 -0.160113 -0.357380   \n",
       "\n",
       "         f8        f9       f10    ...        M_f48     M_f49     M_f50  \\\n",
       "0 -0.003309  1.761018  0.968297    ...     0.007494  0.064775  0.107379   \n",
       "1 -0.002772  1.682896  0.354919    ...     0.005310  0.064775  0.107379   \n",
       "2 -0.012089  1.165470  0.440719    ...     0.007494  0.057598  0.107379   \n",
       "3  0.009897  0.880806  1.230350    ...     0.007494  0.064775  0.107379   \n",
       "4  0.004808  0.481565  1.404777    ...     0.007494  0.064775  0.084504   \n",
       "5  0.001317  0.162080  0.121588    ...     0.007494  0.064775  0.107379   \n",
       "6 -0.002333  0.032518 -0.057125    ...     0.007494  0.064775  0.107379   \n",
       "7 -0.003962 -0.033998  0.032210    ...     0.007059  0.064741  0.107379   \n",
       "8  0.001081  0.481565 -0.507366    ...     0.007494  0.064775  0.107379   \n",
       "9  0.000226 -0.110449 -0.385231    ...     0.007494  0.064775  0.107379   \n",
       "\n",
       "      M_f51     M_f52     M_f53     M_f54     M_f55     M_f56     M_f57  \n",
       "0  0.002549  0.069281  0.092629  0.056297  0.001695  0.001691  0.058936  \n",
       "1  0.002549  0.069281  0.097933  0.056297  0.001695  0.001691  0.058936  \n",
       "2  0.002549  0.069281  0.097933  0.056297  0.001415  0.001412  0.058936  \n",
       "3  0.002549  0.069281  0.097933  0.042310  0.001533  0.001530  0.050184  \n",
       "4  0.002549  0.069281  0.097933  0.056297  0.001695  0.001691  0.058936  \n",
       "5  0.002549  0.055455  0.097933  0.056297  0.001695  0.001691  0.058936  \n",
       "6  0.002517  0.069281  0.097933  0.056297  0.001695  0.001691  0.058936  \n",
       "7  0.002549  0.069281  0.097933  0.056297  0.001695  0.001691  0.058936  \n",
       "8  0.002549  0.069281  0.097933  0.056297  0.001695  0.001691  0.058936  \n",
       "9  0.002549  0.069281  0.097933  0.056297  0.001695  0.001691  0.058936  \n",
       "\n",
       "[10 rows x 120 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f = df.ix[:,u'f1':u'f57'].groupby(by = df['event_id']).apply(mean_max_min)\n",
    "df_f['is1'] = df.ix[:,u'is1'].groupby(by =df['event_id']).first()\n",
    "df_f['oos'] = df.ix[:,u'oos'].groupby(by =df['event_id']).first()\n",
    "for col in df_f.columns[:-2]:\n",
    "    df[str('M_'+col)]= df_f[col].values\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names = ['f%s'%i for i in range(1,58)] + ['M_f%s'%i for i in range(1,58)] +['result', 'event_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_y_data, df_event = ranking_data(df[df.is1], [2], col_names)\n",
    "X_train, y_train = X_y_data[:,:-1], X_y_data[:,-1]\n",
    "X_y_data, df_event = ranking_data(df[df.oos], [2], col_names)\n",
    "X_test, y_test = X_y_data[:,:-1], X_y_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>M_f48</th>\n",
       "      <th>M_f49</th>\n",
       "      <th>M_f50</th>\n",
       "      <th>M_f51</th>\n",
       "      <th>M_f52</th>\n",
       "      <th>M_f53</th>\n",
       "      <th>M_f54</th>\n",
       "      <th>M_f55</th>\n",
       "      <th>M_f56</th>\n",
       "      <th>M_f57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.152068</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.971937</td>\n",
       "      <td>1.096287</td>\n",
       "      <td>-0.003309</td>\n",
       "      <td>1.761018</td>\n",
       "      <td>0.968297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.092629</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.058936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.137530</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.398019</td>\n",
       "      <td>0.481980</td>\n",
       "      <td>-0.002772</td>\n",
       "      <td>1.682896</td>\n",
       "      <td>0.354919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005310</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.058936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.029570</td>\n",
       "      <td>-0.145221</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.307072</td>\n",
       "      <td>0.427015</td>\n",
       "      <td>-0.012089</td>\n",
       "      <td>1.165470</td>\n",
       "      <td>0.440719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.057598</td>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.058936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.136884</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.234280</td>\n",
       "      <td>0.361007</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>0.880806</td>\n",
       "      <td>1.230350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.107379</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.042310</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.050184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.343377</td>\n",
       "      <td>0.242782</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.017472</td>\n",
       "      <td>-0.199782</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.481565</td>\n",
       "      <td>1.404777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.084504</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.069281</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.056297</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.058936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2        f3        f4      f5        f6        f7  \\\n",
       "0  0.064913  1.395654 -0.152068  0.008172  0.0007  0.971937  1.096287   \n",
       "1 -0.250759  1.137530  0.019747  0.008894  0.0007  0.398019  0.481980   \n",
       "2 -0.250759  1.029570 -0.145221 -0.020776  0.0007  0.307072  0.427015   \n",
       "3  0.064913  1.395654 -0.136884 -0.020776  0.0007  0.234280  0.361007   \n",
       "4  0.064913  1.343377  0.242782  0.008894  0.0007 -0.017472 -0.199782   \n",
       "\n",
       "         f8        f9       f10    ...        M_f48     M_f49     M_f50  \\\n",
       "0 -0.003309  1.761018  0.968297    ...     0.007494  0.064775  0.107379   \n",
       "1 -0.002772  1.682896  0.354919    ...     0.005310  0.064775  0.107379   \n",
       "2 -0.012089  1.165470  0.440719    ...     0.007494  0.057598  0.107379   \n",
       "3  0.009897  0.880806  1.230350    ...     0.007494  0.064775  0.107379   \n",
       "4  0.004808  0.481565  1.404777    ...     0.007494  0.064775  0.084504   \n",
       "\n",
       "      M_f51     M_f52     M_f53     M_f54     M_f55     M_f56     M_f57  \n",
       "0  0.002549  0.069281  0.092629  0.056297  0.001695  0.001691  0.058936  \n",
       "1  0.002549  0.069281  0.097933  0.056297  0.001695  0.001691  0.058936  \n",
       "2  0.002549  0.069281  0.097933  0.056297  0.001415  0.001412  0.058936  \n",
       "3  0.002549  0.069281  0.097933  0.042310  0.001533  0.001530  0.050184  \n",
       "4  0.002549  0.069281  0.097933  0.056297  0.001695  0.001691  0.058936  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score   0.552586972314\n",
      "accuracy   0.578704720088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators =100, oob_score=True, n_jobs =-1)\n",
    "forest.fit(X_train, y_train)\n",
    "print 'train score  ',forest.oob_score_\n",
    "print 'accuracy  ', accuracy_score ( y_test, forest.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score   0.589313100331\n",
      "accuracy   0.582217343578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda2\\lib\\site-packages\\sklearn\\discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(X_train, y_train)\n",
    "print 'train score  ',LDA.score(X_train, y_train)\n",
    "print 'accuracy  ', accuracy_score ( y_test, LDA.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score   0.591573169747\n",
      "accuracy   0.511964873765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda2\\lib\\site-packages\\sklearn\\discriminant_analysis.py:695: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "QDA = QuadraticDiscriminantAnalysis()\n",
    "QDA.fit(X_train, y_train)\n",
    "print 'train score  ',QDA.score(X_train, y_train)\n",
    "print 'accuracy  ', accuracy_score ( y_test, QDA.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def P_pair(data, clf):\n",
    "    \n",
    "    \n",
    "    n,m = data.shape\n",
    "    p_matrix = np.zeros((n,n))\n",
    "    np.fill_diagonal(p_matrix, 0.5) \n",
    "    featurs = data.columns[:-2]\n",
    "    for  i,i_ in  enumerate(data.index):\n",
    "    \n",
    "        ar_S_ji = (data.ix[data.index > i_, featurs] - data.ix[i_, featurs]).values\n",
    "        if ar_S_ji.shape[0] >=1:\n",
    "            p_matrix[i,i+1:] = forest.predict_proba(ar_S_ji)[:,0].reshape(1,-1)\n",
    "    \n",
    "    return np.triu(1 -p_matrix, 1).T + p_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04252084,  0.05152016,  0.06530585,  0.04005717,  0.04136367,\n",
       "        0.0651916 ,  0.06061875,  0.06873915,  0.08411466,  0.08938966,\n",
       "        0.13114909,  0.13640713,  0.1021013 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_win = lambda x: 1/(1/x -1).sum(axis =1)\n",
    "P_win_from_pair1 = lambda x: P_win(P_pair(x, forest))#.reshape(-1,1)\n",
    "P_win_from_pair1(df.ix[df.event_id == 287398, col_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def P_win_from_pair(data, clf):\n",
    "    n,m = data.shape\n",
    "    p_matrix = np.zeros((n,n))\n",
    "    np.fill_diagonal(p_matrix, 0.5)\n",
    "    features = data.columns\n",
    "    #print features\n",
    "    for  i,i_ in  enumerate(data.index):\n",
    "    \n",
    "        ar_S_ji = (data.ix[data.index > i_, features] - data.ix[i_, features]).values\n",
    "        if ar_S_ji.shape[0] >=1:\n",
    "            p_matrix[i,i+1:] = forest.predict_proba(ar_S_ji)[:,0].reshape(1,-1)\n",
    "    \n",
    "    P_pair = np.triu(1 -p_matrix, 1).T + p_matrix\n",
    "    #print (1/(1/P_pair -1).sum(axis =1))\n",
    "    return (1/(1/P_pair -1).sum(axis =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_group = df.ix[:99,col_names[:-2]].groupby(by = df.event_id).apply(lambda x: P_win_from_pair(x, forest))\n",
    "P_forest = np.array([])\n",
    "for event in df_group.index:\n",
    "    P_forest = np.hstack((P_forest,df_group[event]))\n",
    "#df['p_forest'] = P_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13586.505050505049"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df.oos)/99.*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_group = df.ix[:,col_names[:-2]].groupby(by = df.event_id).apply(lambda x: P_win_from_pair(x, forest))\n",
    "P_forest = np.array([])\n",
    "for event in df_group.index:\n",
    "    P_forest = np.hstack((P_forest,df_group[event]))\n",
    "df['p_forest'] = P_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'oos  mean ll  ', np.log(df.ix[df.oos & (df.result ==1),'p_forest']).mean()*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations, combinations_with_replacement, permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "\n",
      "accuracy  test 0.571049332645\n",
      "\n",
      "is1  mean ll Logist -2532.7164248\n",
      "\n",
      "oos  mean ll Logist -2574.31632632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "logit = LogisticRegression (C =10., penalty ='l1', n_jobs =-1)\n",
    "df1, logit = clf_llhood(df, logit, [2,3, 4], col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 61, 112, 111, 108,  55,  70,  73,  76,  80,  82,  87,  92,  64,\n",
       "         95,  16,  12,  30,   1,  56,  28,  41,  42,  45,  57,  23,  32,\n",
       "          5,  22,   9,  21,  86,  51,  50,  14,  38,  39,  58,  81,  66,\n",
       "         17,  96,  48,  11,  25,  35,  10,   7,   4,  26,  31,  18,  13,\n",
       "         40,  24,   6,  98,  52,  53,  34,  97,  20,   8,   0,  15, 106,\n",
       "         71,  49,  67,  37, 107,  99,  46,  29,  65,  27,  72, 104,  68,\n",
       "         43,  47,  69,  62,  19, 109,   3,  33,  54,   2,  36,  63, 102,\n",
       "         84,  79,  60,  78, 105,  89,  94, 103,  75,  85,  83,  93,  44,\n",
       "         90, 100,  88, 110, 113,  59,  74,  77,  91, 101]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(logit.coef_).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 55, 61, 64, 70, 73, 76, 80, 82, 87, 92, 95, 108, 111, 112]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i,f in enumerate((np.abs(logit.coef_) == 0.0)[0,:]) if f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LinearSVC(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "\n",
      "\n",
      "\n",
      "accuracy  test 0.570606887398\n",
      "\n",
      "is1  mean ll Linear -2371.90506774\n",
      "\n",
      "oos  mean ll Linear -2407.64531481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(C = 10.0, dual =False, penalty='l1')\n",
    "df1,svm = clf_llhood(df, svm, [2, 3, 4], col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[111,  57,   1,  87,  12,  56,  30,  41,  42,  28,  64,  95,  96,\n",
       "         22,  32,  45,  23,  14,   5,   9,  21,  66,  50,  39,  51,  58,\n",
       "         92,  38,  48,  17,  11,  80,  10,  35,  40,  31,  55,  26,  16,\n",
       "         25,   4,  70,  18, 108,  13,  24,  98,  97,  53,  52,   6,  81,\n",
       "         76,   8,  20,  54,   0,  86,  15,   7,  49,  37, 106,  99,  65,\n",
       "         67,  46,  29,  27,  71,  68, 107,  47,  43,  34,  19,  62,  72,\n",
       "         69, 109, 102,   3,  61, 104,  33,   2,  79,  63,  36,  89,  78,\n",
       "         83,  94, 103,  60, 105,  84,  44,  85,  75, 110,  90, 100,  88,\n",
       "         93, 113,  59,  74,  73, 112,  77,  82, 101,  91]], dtype=int64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(svm.coef_).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57, 111]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i,f in enumerate((np.abs(svm.coef_) == 0.0)[0,:]) if f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators =200, max_depth =2, loss ='exponential')\n",
    "df1, gbc = clf_llhood(df, gbc, [2, 3, 4], col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=100, random_state=None)\n",
      "\n",
      "accuracy  test 0.578265642151\n",
      "\n",
      "is1  mean ll AdaBoo -2256.8277632\n",
      "\n",
      "oos  mean ll AdaBoo -2287.55956968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier(n_estimators=100)\n",
    "df1, abc = clf_llhood(df, abc, [2], col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30,  62,  31,  34,  81,  80,  79,  38,  77,  76,  41,  42,  73,\n",
       "        71,  70,  48,  50,  51,  66,  55, 112,  63,  58,  59,  85,  28,\n",
       "        61,  17, 103, 107, 100,  26,  98,  97,  16,  95, 104,  20,  93,\n",
       "         4, 109, 110,  91,  88, 111,  94, 105, 108,  64,  67,  86,  74,\n",
       "       102, 101,  75,  96,  82,  84,  92,  87,  69,  56, 113,   8,  22,\n",
       "        18,  14,  60,  25,  27,  29,  13,  12,  35,   9,  21,  39,  19,\n",
       "        43,  54,   2,  53,  52,   3,  40,   5,   6,   7,  44,  47,  99,\n",
       "       106,  15,  10,  57,  24,  90,  65,  68,  49,  72,  23,   0,  33,\n",
       "        83,  89,  36,  37,  11,  45,  46,   1,  78,  32], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.feature_importances_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.040000000000000001)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.feature_importances_[30], abc.feature_importances_[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 79,  55,  91, 101,  64,  10,  16,  34,  63,  38,  73,  68, 104,\n",
       "        85, 111, 102,  84,  18, 108,  94,  95,  74,  54,  92,  30,  88,\n",
       "         4,  81,  76,  75,  97,  71,  60,  45,  70, 105,  24, 112,   7,\n",
       "        86,  40,  61,  58,  48,  57,  36,  98,  35,  72, 109, 106,  62,\n",
       "        50,   5,  93,  33,  83,  78,  28,  23,  26,  82,  22, 100,   6,\n",
       "        89,  69,  41, 107, 110,  67,  80,  96,  90, 103,  87,   8,  12,\n",
       "        59,  11,  17, 113,  21,  77,   2,  52,  42,  44,  53,   0,  51,\n",
       "        46,  56,   9,  47,  13,  66,  14,  65,  43,  20,   3,  27,  99,\n",
       "        31,  15,  19,  25,  29,  32,  49,  39,  37,   1], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.feature_importances_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)\n",
      "\n",
      "accuracy  test 0.582217343578\n",
      "\n",
      "is1  mean ll Linear -2582.48453657\n",
      "\n",
      "oos  mean ll Linear -2628.42004762\n"
     ]
    }
   ],
   "source": [
    "df1 = clf_llhood(df, LDA, [2], col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n",
      "               store_covariances=False, tol=0.0001)\n",
      "\n",
      "accuracy  test 0.501866081229\n",
      "\n",
      "is1  mean ll Quadra -inf\n",
      "\n",
      "oos  mean ll Quadra -9045.99619344\n"
     ]
    }
   ],
   "source": [
    "df1 = clf_llhood(df, QDA, [2], col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = np.random.randint(len(X_train), size=(12,1000))\n",
    "logits = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train  score   0.629\n",
      "test Score   0.566630076839\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train  score   0.619\n",
      "test Score   0.56882546652\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train  score   0.607\n",
      "test Score   0.572338090011\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train  score   0.628\n",
      "test Score   0.56641053787\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train  score   0.643\n",
      "test Score   0.572777167947\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train  score   0.62\n",
      "test Score   0.552579582876\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train  score   0.647\n",
      "test Score   0.565971459934\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train  score   0.635\n",
      "test Score   0.549945115258\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train  score   0.61\n",
      "test Score   0.572777167947\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train  score   0.626\n",
      "test Score   0.572338090011\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train  score   0.626\n",
      "test Score   0.552360043908\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train  score   0.627\n",
      "test Score   0.56641053787\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ind)):\n",
    "    print logit.fit(X_train[ind[i]], y_train[ind[i]])\n",
    "    logits.append(logit)\n",
    "    print 'train  score  ',logit.score(X_train[ind[i]], y_train[ind[i]])\n",
    "    print 'test Score  ', accuracy_score(y_test, logit.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_data, df_event = ranking_data(df[df.is1], [3], col_names)\n",
    "X_train, y_train = X_y_data[:,:-1], X_y_data[:,-1]\n",
    "X_y_data, df_event = ranking_data(df[df.oos], [3], col_names)\n",
    "X_test, y_test = X_y_data[:,:-1], X_y_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_tr = np.zeros((X_train.shape[0], len(logits)))\n",
    "X_new_ts = np.zeros((X_test.shape[0], len(logits)))\n",
    "for i, clf in enumerate(logits):\n",
    "    X_new_tr[:, i] = clf.predict_proba(X_train)[:,0]\n",
    "    X_new_ts[:, i] = clf.predict_proba(X_test)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12389L, 1L)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_tr.mean(axis =1).reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_log = np.log(X_new_tr.mean(axis =1).reshape(-1,1))\n",
    "X_tr_log_1 = np.log(1-X_new_tr.mean(axis =1).reshape(-1,1))\n",
    "X_ts_log = np.log(X_new_ts.mean(axis =1).reshape(-1,1))\n",
    "X_ts_log_1 = np.log(1-X_new_ts.mean(axis =1).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_add = np.hstack((X_train, X_new_tr.mean(axis =1).reshape(-1,1)))\n",
    "X_train_add = np.hstack((X_train_add, 1 -X_new_tr.mean(axis =1).reshape(-1,1)))\n",
    "X_train_add = np.hstack((X_train_add, X_tr_log))\n",
    "X_train_add = np.hstack((X_train_add, X_tr_log_1))\n",
    "X_test_add = np.hstack((X_test, X_new_ts.mean(axis =1).reshape(-1,1)))\n",
    "X_test_add = np.hstack((X_test_add, 1 -X_new_ts.mean(axis =1).reshape(-1,1)))\n",
    "X_test_add = np.hstack((X_test_add, X_ts_log))\n",
    "X_test_add = np.hstack((X_test_add, X_ts_log_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "train score   0.57438049883\n",
      "\n",
      "test score   0.556531284303\n"
     ]
    }
   ],
   "source": [
    "print logit.fit(X_train_add, y_train)\n",
    "print 'train score  ',logit.score(X_train_add, y_train)\n",
    "print \n",
    "print 'test score  ', accuracy_score(y_test, logit.predict(X_test_add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.0001, verbose=0)\n",
      "train score   0.57090967794\n",
      "\n",
      "test score   0.559385290889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(C = 1.0, loss ='hinge')\n",
    "print svm.fit(X_train_add, y_train)\n",
    "print 'train score  ',svm.score(X_train_add, y_train)\n",
    "print \n",
    "print 'test score  ', accuracy_score(y_test, svm.predict(X_test_add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=100, presort='auto', random_state=None,\n",
      "              subsample=1.0, verbose=0, warm_start=False)\n",
      "train score   0.643151182501\n",
      "\n",
      "test score   0.550384193194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "print gbc.fit(X_train_add, y_train)\n",
    "print 'train score  ',gbc.score(X_train_add, y_train)\n",
    "print \n",
    "print 'test score  ', accuracy_score(y_test, gbc.predict(X_test_add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=100, random_state=None)\n",
      "train score   0.61514246509\n",
      "\n",
      "test score   0.547969264544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier(n_estimators=100)\n",
    "print abc.fit(X_train_add, y_train)\n",
    "print 'train score  ',abc.score(X_train_add, y_train)\n",
    "print \n",
    "print 'test score  ', accuracy_score(y_test, abc.predict(X_test_add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=100, p=2,\n",
      "           weights='uniform')\n",
      "train score   0.575349100008\n",
      "\n",
      "test score   0.550164654226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kMM = KNeighborsClassifier (n_neighbors =100, n_jobs =-1)\n",
    "print kMM.fit(X_train_add, y_train)\n",
    "print 'train score  ',kMM.score(X_train_add, y_train)\n",
    "print \n",
    "print 'test score  ', accuracy_score(y_test, kMM.predict(X_test_add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=-1, oob_score=True, random_state=7,\n",
      "            verbose=0, warm_start=False)\n",
      "train score   0.545161029946\n",
      "\n",
      "test score   0.549506037322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators =100, random_state = 7, oob_score=True, n_jobs =-1)\n",
    "print forest.fit(X_train_add, y_train)\n",
    "print 'train score  ',forest.oob_score_\n",
    "print \n",
    "print 'test score  ', accuracy_score(y_test, forest.predict(X_test_add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "print LDA.fit(X_train_add, y_train)\n",
    "print \n",
    "print 'train score  ',LDA.score(X_train_add, y_train)\n",
    "print \n",
    "print 'test score  ', accuracy_score(y_test, LDA.predict(X_test_add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = forest.predict_proba(X_test_add)[:,0] + logit.predict_proba(X_test_add)[:,0] +\\\n",
    "gbc.predict_proba(X_test_add)[:,0] + abc.predict_proba(X_test_add)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55828759604829858"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, np.where(mean/4 >= 0.5, -1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- дополнительные признаки квадраты факторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>2f52</th>\n",
       "      <th>2f53</th>\n",
       "      <th>2f54</th>\n",
       "      <th>2f55</th>\n",
       "      <th>2f56</th>\n",
       "      <th>2f57</th>\n",
       "      <th>result</th>\n",
       "      <th>event_id</th>\n",
       "      <th>is1</th>\n",
       "      <th>oos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.152068</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.971937</td>\n",
       "      <td>1.096287</td>\n",
       "      <td>-0.003309</td>\n",
       "      <td>1.761018</td>\n",
       "      <td>0.968297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115769</td>\n",
       "      <td>0.382722</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>3</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.137530</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.398019</td>\n",
       "      <td>0.481980</td>\n",
       "      <td>-0.002772</td>\n",
       "      <td>1.682896</td>\n",
       "      <td>0.354919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028750</td>\n",
       "      <td>0.160713</td>\n",
       "      <td>0.032101</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.045372</td>\n",
       "      <td>5</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.029570</td>\n",
       "      <td>-0.145221</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.307072</td>\n",
       "      <td>0.427015</td>\n",
       "      <td>-0.012089</td>\n",
       "      <td>1.165470</td>\n",
       "      <td>0.440719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097088</td>\n",
       "      <td>0.184932</td>\n",
       "      <td>0.047268</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.029932</td>\n",
       "      <td>13</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.136884</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.234280</td>\n",
       "      <td>0.361007</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>0.880806</td>\n",
       "      <td>1.230350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020284</td>\n",
       "      <td>0.302158</td>\n",
       "      <td>0.159399</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.106788</td>\n",
       "      <td>1</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.343377</td>\n",
       "      <td>0.242782</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.017472</td>\n",
       "      <td>-0.199782</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.481565</td>\n",
       "      <td>1.404777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056028</td>\n",
       "      <td>0.053604</td>\n",
       "      <td>0.105756</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.021267</td>\n",
       "      <td>6</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2        f3        f4      f5        f6        f7  \\\n",
       "0  0.064913  1.395654 -0.152068  0.008172  0.0007  0.971937  1.096287   \n",
       "1 -0.250759  1.137530  0.019747  0.008894  0.0007  0.398019  0.481980   \n",
       "2 -0.250759  1.029570 -0.145221 -0.020776  0.0007  0.307072  0.427015   \n",
       "3  0.064913  1.395654 -0.136884 -0.020776  0.0007  0.234280  0.361007   \n",
       "4  0.064913  1.343377  0.242782  0.008894  0.0007 -0.017472 -0.199782   \n",
       "\n",
       "         f8        f9       f10  ...        2f52      2f53      2f54  \\\n",
       "0 -0.003309  1.761018  0.968297  ...    0.115769  0.382722  0.001619   \n",
       "1 -0.002772  1.682896  0.354919  ...    0.028750  0.160713  0.032101   \n",
       "2 -0.012089  1.165470  0.440719  ...    0.097088  0.184932  0.047268   \n",
       "3  0.009897  0.880806  1.230350  ...    0.020284  0.302158  0.159399   \n",
       "4  0.004808  0.481565  1.404777  ...    0.056028  0.053604  0.105756   \n",
       "\n",
       "       2f55      2f56      2f57  result  event_id   is1    oos  \n",
       "0  0.000012  0.000011  0.001353       3    287398  True  False  \n",
       "1  0.000008  0.000008  0.045372       5    287398  True  False  \n",
       "2  0.000150  0.000147  0.029932      13    287398  True  False  \n",
       "3  0.000096  0.000098  0.106788       1    287398  True  False  \n",
       "4  0.000022  0.000023  0.021267       6    287398  True  False  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = ['f%s'%i for i in range(1,58)]+['2f%s'%i for i in range(1,58)]\n",
    "df2 = pd.DataFrame(data= np.hstack((df.ix[:,u'f1':u'f57'].values, df.ix[:,u'f1':u'f57'].values*df.ix[:,u'f1':u'f57'].values)), \n",
    "              columns = col_names)\n",
    "for col in ['result', 'event_id', 'is1', 'oos']:\n",
    "    df2[col] =df[col]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_llhood (df2, logit, result =[2], columns = col_names +['result','event_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_y_data, df_event = ranking_data(df[df.is1], [4])\n",
    "X_train, y_train = X_y_data[:,:-1], X_y_data[:,-1]\n",
    "X_y_data, df_event = ranking_data(df[df.oos], [4])\n",
    "X_test, y_test = X_y_data[:,:-1], X_y_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12081L, 1L), (12081L, 57L))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.predict_log_proba(X_train)[:,:1].shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_log_p  = np.hstack((X_train, logit.predict_log_proba(X_train)[:,:1]))\n",
    "X_ts_log_p  = np.hstack((X_test, logit.predict_log_proba(X_test)[:,:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit1 = LogisticRegression (C =1., penalty ='l2', n_jobs =-1, class_weight ='balanced')\n",
    "logit1.fit(X_tr_log_p, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5569903153712441"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit1.score(X_tr_log_p, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55380813300381937"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, logit1.predict(X_ts_log_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "accuracy   0.590998902305\n",
      "\n",
      "is1  mean ll  -2610.25367312\n",
      "\n",
      "oos  mean ll   -2661.45364861\n"
     ]
    }
   ],
   "source": [
    "clf_llhood(df, logit, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "accuracy   0.591877058178\n",
      "\n",
      "is1  mean ll  -2396.41145935\n",
      "\n",
      "oos  mean ll   -2435.96723734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(C = 1.0 )\n",
    "clf_llhood(df, svm, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S_train = X_train.dot(logit.coef_[0])\n",
    "S_test = X_test.dot(logit.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.400385377827603, 0.00012600539310712866)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_max = np.abs(S_train).max()\n",
    "np.abs(S_train).max(), np.abs(S_train).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_S = X_train*(S_max/ S_train).reshape(-1,1)\n",
    "X_test_S = X_test*(S_max/ S_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51030543829153219"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit1.fit(X_train_S, y_train)\n",
    "logit1.score(X_train_S, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49494495618962031"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, logit1.predict(X_test_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_data, df_event = ranking_data(df[df.is1], [2])\n",
    "X_train, y_train = X_y_data[:,:-1], X_y_data[:,-1]\n",
    "X_y_data, df_event_ts = ranking_data(df[df.oos], [2])\n",
    "X_test, y_test = X_y_data[:,:-1], X_y_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "BGM = BayesianGaussianMixture(n_components=6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianGaussianMixture(covariance_prior=None, covariance_type='full',\n",
       "            degrees_of_freedom_prior=None, init_params='kmeans',\n",
       "            max_iter=100, mean_precision_prior=None, mean_prior=None,\n",
       "            n_components=6, n_init=1, random_state=None, reg_covar=1e-06,\n",
       "            tol=0.001, verbose=0, verbose_interval=10, warm_start=False,\n",
       "            weight_concentration_prior=None,\n",
       "            weight_concentration_prior_type='dirichlet_process')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BGM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cl = BGM.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_P_train= BGM.predict_proba(X_train)\n",
    "cluster_P_test= BGM.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = pd.DataFrame(data = BGM.predict(X_train), index = df_event.values, columns =[['cl']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in np.unique(df_cluster['cl']):\n",
    "    df_cluster['cl_%s'%n] = cluster_P_train[:, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cl</th>\n",
       "      <th>cl_0</th>\n",
       "      <th>cl_1</th>\n",
       "      <th>cl_2</th>\n",
       "      <th>cl_3</th>\n",
       "      <th>cl_4</th>\n",
       "      <th>cl_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287398</th>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287399</th>\n",
       "      <td>1</td>\n",
       "      <td>1.277027e-185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.165435e-09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.345455e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287400</th>\n",
       "      <td>1</td>\n",
       "      <td>4.148036e-49</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.510896e-10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.766541e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287401</th>\n",
       "      <td>2</td>\n",
       "      <td>1.452970e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.999855e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.725975e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287402</th>\n",
       "      <td>1</td>\n",
       "      <td>1.100479e-18</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>6.510512e-07</td>\n",
       "      <td>9.626145e-291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.947819e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cl           cl_0      cl_1          cl_2           cl_3  cl_4  \\\n",
       "287398   1   1.552685e-99  1.000000  3.280995e-09  4.502352e-271   0.0   \n",
       "287399   1  1.277027e-185  1.000000  1.165435e-09   0.000000e+00   0.0   \n",
       "287400   1   4.148036e-49  1.000000  9.510896e-10   0.000000e+00   0.0   \n",
       "287401   2   1.452970e-05  0.000000  9.999855e-01   0.000000e+00   0.0   \n",
       "287402   1   1.100479e-18  0.999999  6.510512e-07  9.626145e-291   0.0   \n",
       "\n",
       "                cl_5  \n",
       "287398  3.840932e-10  \n",
       "287399  5.345455e-10  \n",
       "287400  6.766541e-08  \n",
       "287401  4.725975e-23  \n",
       "287402  2.947819e-13  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cluster_ts = pd.DataFrame(data = BGM.predict(X_test), index = df_event_ts.values, columns =[['cl']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in np.unique(df_cluster_ts['cl']):\n",
    "    df_cluster_ts['cl_%s'%n] = cluster_P_test[:, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster  0,  count 3732\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "train score   0.595391211147\n",
      "\n",
      "cluster  1,  count 5563\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "train score   0.582060039547\n",
      "\n",
      "cluster  2,  count 1426\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "train score   0.654978962132\n",
      "\n",
      "cluster  3,  count 20\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "train score   0.95\n",
      "\n",
      "cluster  4,  count 11\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "train score   1.0\n",
      "\n",
      "cluster  5,  count 1637\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "train score   0.593158216249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cl_logit =[]\n",
    "for n in np.unique(df_cluster['cl']):\n",
    "    \n",
    "    logit1 = LogisticRegression (C =1., penalty ='l2', n_jobs =-1, class_weight ='balanced')\n",
    "    X_tr_cl = X_train[(df_cluster['cl'] == n).values]\n",
    "    y_tr_cl = y_train[(df_cluster['cl'] == n).values]\n",
    "    print 'cluster  %s,  count %s'%(n,len(y_tr_cl))\n",
    "    print logit1.fit(X_tr_cl, y_tr_cl)\n",
    "    print 'train score  ',logit1.score(X_tr_cl, y_tr_cl)\n",
    "    print\n",
    "    cl_logit.append(logit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = np.zeros((X_train.shape[0], 2))\n",
    "for n in np.unique(df_cluster['cl']): \n",
    "    proba = proba +cl_logit[n].predict_proba(X_train)* df_cluster['cl_%s'%n].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proba_ts = np.zeros((X_test.shape[0], 2))\n",
    "for n in np.unique(df_cluster_ts['cl']): \n",
    "    proba_ts = proba_ts +cl_logit[n].predict_proba(X_test)* df_cluster_ts['cl_%s'%n].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5966583259342966"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_tr = np.ones((len(proba)))\n",
    "predict_tr = np.where(np.argmax(proba, axis = 1) == 0, -1, 1)\n",
    "1.*((y_train - predict_tr) == 0).sum()/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5815587266739847"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ts = np.ones((len(proba_ts)))\n",
    "predict_ts = np.where(np.argmax(proba_ts, axis = 1) == 0, -1, 1)\n",
    "1.*((y_test - predict_ts) == 0).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16944, 7)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cluster_add = df_cluster.append(df_cluster_ts)\n",
    "df_cluster_add.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>f32</th>\n",
       "      <th>f33</th>\n",
       "      <th>f34</th>\n",
       "      <th>f35</th>\n",
       "      <th>f36</th>\n",
       "      <th>f37</th>\n",
       "      <th>f38</th>\n",
       "      <th>f39</th>\n",
       "      <th>f40</th>\n",
       "      <th>f41</th>\n",
       "      <th>f42</th>\n",
       "      <th>f43</th>\n",
       "      <th>f44</th>\n",
       "      <th>f45</th>\n",
       "      <th>f46</th>\n",
       "      <th>f47</th>\n",
       "      <th>f48</th>\n",
       "      <th>f49</th>\n",
       "      <th>f50</th>\n",
       "      <th>f51</th>\n",
       "      <th>f52</th>\n",
       "      <th>f53</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>runner_id</th>\n",
       "      <th>result</th>\n",
       "      <th>event_id</th>\n",
       "      <th>is1</th>\n",
       "      <th>is2</th>\n",
       "      <th>oos</th>\n",
       "      <th>S</th>\n",
       "      <th>p</th>\n",
       "      <th>log_p</th>\n",
       "      <th>cl_x</th>\n",
       "      <th>cl_0_x</th>\n",
       "      <th>cl_1_x</th>\n",
       "      <th>cl_2_x</th>\n",
       "      <th>cl_3_x</th>\n",
       "      <th>cl_4_x</th>\n",
       "      <th>cl_5_x</th>\n",
       "      <th>S_cl_0</th>\n",
       "      <th>p_cl__0</th>\n",
       "      <th>S_cl_1</th>\n",
       "      <th>p_cl__1</th>\n",
       "      <th>S_cl_2</th>\n",
       "      <th>p_cl__2</th>\n",
       "      <th>S_cl_3</th>\n",
       "      <th>p_cl__3</th>\n",
       "      <th>S_cl_4</th>\n",
       "      <th>p_cl__4</th>\n",
       "      <th>S_cl_5</th>\n",
       "      <th>p_cl__5</th>\n",
       "      <th>p_mix</th>\n",
       "      <th>cl_y</th>\n",
       "      <th>cl_0_y</th>\n",
       "      <th>cl_1_y</th>\n",
       "      <th>cl_2_y</th>\n",
       "      <th>cl_3_y</th>\n",
       "      <th>cl_4_y</th>\n",
       "      <th>cl_5_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.152068</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.971937</td>\n",
       "      <td>1.096287</td>\n",
       "      <td>-0.003309</td>\n",
       "      <td>1.761018</td>\n",
       "      <td>0.968297</td>\n",
       "      <td>0.374753</td>\n",
       "      <td>0.675330</td>\n",
       "      <td>0.051377</td>\n",
       "      <td>-0.005242</td>\n",
       "      <td>1.305105</td>\n",
       "      <td>0.083228</td>\n",
       "      <td>-0.003275</td>\n",
       "      <td>0.032186</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>0.041683</td>\n",
       "      <td>0.015889</td>\n",
       "      <td>-0.358249</td>\n",
       "      <td>0.287939</td>\n",
       "      <td>0.037986</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>-0.001490</td>\n",
       "      <td>-0.472996</td>\n",
       "      <td>-0.003019</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>-0.027652</td>\n",
       "      <td>-0.123439</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.109841</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>-0.002725</td>\n",
       "      <td>-0.215236</td>\n",
       "      <td>-0.173222</td>\n",
       "      <td>0.859616</td>\n",
       "      <td>-0.003327</td>\n",
       "      <td>1.886817</td>\n",
       "      <td>0.450763</td>\n",
       "      <td>0.508958</td>\n",
       "      <td>1.094065</td>\n",
       "      <td>-0.001936</td>\n",
       "      <td>0.065516</td>\n",
       "      <td>0.386516</td>\n",
       "      <td>0.049406</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>0.490221</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>-0.340249</td>\n",
       "      <td>0.618645</td>\n",
       "      <td>0.040241</td>\n",
       "      <td>-0.003450</td>\n",
       "      <td>-0.003327</td>\n",
       "      <td>0.036781</td>\n",
       "      <td>353046</td>\n",
       "      <td>3</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.331537</td>\n",
       "      <td>0.053286</td>\n",
       "      <td>-3.310580</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.678621</td>\n",
       "      <td>0.032506</td>\n",
       "      <td>-0.667135</td>\n",
       "      <td>0.032394</td>\n",
       "      <td>-0.861541</td>\n",
       "      <td>0.028221</td>\n",
       "      <td>-1.299192</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.351216</td>\n",
       "      <td>0.088811</td>\n",
       "      <td>-0.596701</td>\n",
       "      <td>0.036495</td>\n",
       "      <td>0.036495</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.137530</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.398019</td>\n",
       "      <td>0.481980</td>\n",
       "      <td>-0.002772</td>\n",
       "      <td>1.682896</td>\n",
       "      <td>0.354919</td>\n",
       "      <td>-0.994794</td>\n",
       "      <td>0.212441</td>\n",
       "      <td>0.484803</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>1.408825</td>\n",
       "      <td>0.066311</td>\n",
       "      <td>-0.002738</td>\n",
       "      <td>-0.017447</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>-0.012072</td>\n",
       "      <td>-0.023179</td>\n",
       "      <td>0.047898</td>\n",
       "      <td>-0.533036</td>\n",
       "      <td>-0.086651</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.075571</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>-0.044777</td>\n",
       "      <td>-0.020852</td>\n",
       "      <td>0.433588</td>\n",
       "      <td>-0.145489</td>\n",
       "      <td>0.176984</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>-0.002182</td>\n",
       "      <td>0.030317</td>\n",
       "      <td>0.034727</td>\n",
       "      <td>1.008091</td>\n",
       "      <td>-0.002790</td>\n",
       "      <td>1.810774</td>\n",
       "      <td>0.289262</td>\n",
       "      <td>0.620846</td>\n",
       "      <td>0.491455</td>\n",
       "      <td>0.137752</td>\n",
       "      <td>-0.020472</td>\n",
       "      <td>0.191353</td>\n",
       "      <td>-0.296367</td>\n",
       "      <td>-0.051450</td>\n",
       "      <td>-0.250389</td>\n",
       "      <td>-0.015683</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>-0.400890</td>\n",
       "      <td>0.179168</td>\n",
       "      <td>-0.002913</td>\n",
       "      <td>-0.002791</td>\n",
       "      <td>0.213007</td>\n",
       "      <td>352217</td>\n",
       "      <td>5</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.213620</td>\n",
       "      <td>0.059955</td>\n",
       "      <td>-3.546679</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.579957</td>\n",
       "      <td>0.035877</td>\n",
       "      <td>-0.805975</td>\n",
       "      <td>0.028195</td>\n",
       "      <td>-0.281796</td>\n",
       "      <td>0.050391</td>\n",
       "      <td>-1.325842</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>1.418878</td>\n",
       "      <td>0.258312</td>\n",
       "      <td>-0.832801</td>\n",
       "      <td>0.028820</td>\n",
       "      <td>0.028820</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.029570</td>\n",
       "      <td>-0.145221</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.307072</td>\n",
       "      <td>0.427015</td>\n",
       "      <td>-0.012089</td>\n",
       "      <td>1.165470</td>\n",
       "      <td>0.440719</td>\n",
       "      <td>0.135928</td>\n",
       "      <td>0.519989</td>\n",
       "      <td>0.158056</td>\n",
       "      <td>0.013730</td>\n",
       "      <td>0.722187</td>\n",
       "      <td>0.170334</td>\n",
       "      <td>-0.012058</td>\n",
       "      <td>-0.011141</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>-0.013402</td>\n",
       "      <td>-0.022607</td>\n",
       "      <td>-0.209299</td>\n",
       "      <td>-0.519305</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>-0.251839</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.048360</td>\n",
       "      <td>-0.034847</td>\n",
       "      <td>-0.469892</td>\n",
       "      <td>-0.054636</td>\n",
       "      <td>-0.129261</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>-0.011611</td>\n",
       "      <td>-0.194838</td>\n",
       "      <td>-0.143116</td>\n",
       "      <td>0.357578</td>\n",
       "      <td>-0.012107</td>\n",
       "      <td>1.689672</td>\n",
       "      <td>-0.294596</td>\n",
       "      <td>0.774879</td>\n",
       "      <td>0.741813</td>\n",
       "      <td>0.082078</td>\n",
       "      <td>-0.047917</td>\n",
       "      <td>0.386516</td>\n",
       "      <td>-0.296367</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.583518</td>\n",
       "      <td>0.034111</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>-0.311590</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.217412</td>\n",
       "      <td>-0.012249</td>\n",
       "      <td>-0.012108</td>\n",
       "      <td>-0.173009</td>\n",
       "      <td>342962</td>\n",
       "      <td>13</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.094424</td>\n",
       "      <td>0.067545</td>\n",
       "      <td>-2.829194</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.152950</td>\n",
       "      <td>0.054987</td>\n",
       "      <td>-0.368198</td>\n",
       "      <td>0.043681</td>\n",
       "      <td>-0.167563</td>\n",
       "      <td>0.056489</td>\n",
       "      <td>-0.195104</td>\n",
       "      <td>0.033859</td>\n",
       "      <td>-0.124884</td>\n",
       "      <td>0.055169</td>\n",
       "      <td>-0.115315</td>\n",
       "      <td>0.059060</td>\n",
       "      <td>0.059060</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.136884</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.234280</td>\n",
       "      <td>0.361007</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>0.880806</td>\n",
       "      <td>1.230350</td>\n",
       "      <td>0.374753</td>\n",
       "      <td>0.212441</td>\n",
       "      <td>0.584892</td>\n",
       "      <td>0.025233</td>\n",
       "      <td>1.098121</td>\n",
       "      <td>0.119076</td>\n",
       "      <td>0.009932</td>\n",
       "      <td>0.020942</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>-0.012072</td>\n",
       "      <td>0.015889</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>0.553817</td>\n",
       "      <td>0.053025</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>-0.003080</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>-0.002474</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>0.086458</td>\n",
       "      <td>0.020163</td>\n",
       "      <td>0.200410</td>\n",
       "      <td>0.091943</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>-0.073015</td>\n",
       "      <td>-0.061893</td>\n",
       "      <td>0.557530</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>1.360255</td>\n",
       "      <td>0.365248</td>\n",
       "      <td>-0.333936</td>\n",
       "      <td>0.307492</td>\n",
       "      <td>-0.008507</td>\n",
       "      <td>-0.056499</td>\n",
       "      <td>0.191353</td>\n",
       "      <td>0.074037</td>\n",
       "      <td>-0.007525</td>\n",
       "      <td>0.490221</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>-0.142420</td>\n",
       "      <td>0.549689</td>\n",
       "      <td>0.399248</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.326784</td>\n",
       "      <td>361432</td>\n",
       "      <td>1</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.256826</td>\n",
       "      <td>0.057420</td>\n",
       "      <td>-3.334780</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.460113</td>\n",
       "      <td>0.040445</td>\n",
       "      <td>-0.512950</td>\n",
       "      <td>0.037794</td>\n",
       "      <td>-0.503278</td>\n",
       "      <td>0.040380</td>\n",
       "      <td>-1.607062</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.510964</td>\n",
       "      <td>0.104194</td>\n",
       "      <td>-0.620902</td>\n",
       "      <td>0.035622</td>\n",
       "      <td>0.035622</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.343377</td>\n",
       "      <td>0.242782</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.017472</td>\n",
       "      <td>-0.199782</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.481565</td>\n",
       "      <td>1.404777</td>\n",
       "      <td>0.326130</td>\n",
       "      <td>0.248326</td>\n",
       "      <td>-0.371251</td>\n",
       "      <td>-0.005610</td>\n",
       "      <td>1.051047</td>\n",
       "      <td>-0.331326</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.043404</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>0.009731</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.072090</td>\n",
       "      <td>0.519236</td>\n",
       "      <td>0.020156</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.297389</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.020482</td>\n",
       "      <td>0.186001</td>\n",
       "      <td>0.280844</td>\n",
       "      <td>0.262483</td>\n",
       "      <td>0.256026</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.005474</td>\n",
       "      <td>0.445244</td>\n",
       "      <td>0.233803</td>\n",
       "      <td>0.536495</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>1.212282</td>\n",
       "      <td>0.253425</td>\n",
       "      <td>0.327412</td>\n",
       "      <td>-0.213843</td>\n",
       "      <td>-0.013135</td>\n",
       "      <td>0.046403</td>\n",
       "      <td>-0.374462</td>\n",
       "      <td>0.049406</td>\n",
       "      <td>0.031352</td>\n",
       "      <td>0.108361</td>\n",
       "      <td>0.665547</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>0.236702</td>\n",
       "      <td>0.231525</td>\n",
       "      <td>-0.325201</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>-0.145831</td>\n",
       "      <td>359420</td>\n",
       "      <td>6</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.156319</td>\n",
       "      <td>0.063491</td>\n",
       "      <td>-2.936739</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.748355</td>\n",
       "      <td>0.030317</td>\n",
       "      <td>-0.013145</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>-0.315810</td>\n",
       "      <td>0.048706</td>\n",
       "      <td>0.076846</td>\n",
       "      <td>0.044441</td>\n",
       "      <td>0.736712</td>\n",
       "      <td>0.130582</td>\n",
       "      <td>-0.222861</td>\n",
       "      <td>0.053038</td>\n",
       "      <td>0.053038</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2        f3        f4      f5        f6        f7  \\\n",
       "0  0.064913  1.395654 -0.152068  0.008172  0.0007  0.971937  1.096287   \n",
       "1 -0.250759  1.137530  0.019747  0.008894  0.0007  0.398019  0.481980   \n",
       "2 -0.250759  1.029570 -0.145221 -0.020776  0.0007  0.307072  0.427015   \n",
       "3  0.064913  1.395654 -0.136884 -0.020776  0.0007  0.234280  0.361007   \n",
       "4  0.064913  1.343377  0.242782  0.008894  0.0007 -0.017472 -0.199782   \n",
       "\n",
       "         f8        f9       f10       f11       f12       f13       f14  \\\n",
       "0 -0.003309  1.761018  0.968297  0.374753  0.675330  0.051377 -0.005242   \n",
       "1 -0.002772  1.682896  0.354919 -0.994794  0.212441  0.484803  0.010791   \n",
       "2 -0.012089  1.165470  0.440719  0.135928  0.519989  0.158056  0.013730   \n",
       "3  0.009897  0.880806  1.230350  0.374753  0.212441  0.584892  0.025233   \n",
       "4  0.004808  0.481565  1.404777  0.326130  0.248326 -0.371251 -0.005610   \n",
       "\n",
       "        f15       f16       f17       f18      f19       f20       f21  \\\n",
       "0  1.305105  0.083228 -0.003275  0.032186  0.00289  0.041683  0.015889   \n",
       "1  1.408825  0.066311 -0.002738 -0.017447  0.00289 -0.012072 -0.023179   \n",
       "2  0.722187  0.170334 -0.012058 -0.011141  0.00289 -0.013402 -0.022607   \n",
       "3  1.098121  0.119076  0.009932  0.020942  0.00289 -0.012072  0.015889   \n",
       "4  1.051047 -0.331326  0.004842  0.043404  0.00289  0.009731  0.004371   \n",
       "\n",
       "        f22       f23       f24       f25       f26       f27       f28  \\\n",
       "0 -0.358249  0.287939  0.037986 -0.008733 -0.001490 -0.472996 -0.003019   \n",
       "1  0.047898 -0.533036 -0.086651 -0.008733  0.000512  0.075571  0.000948   \n",
       "2 -0.209299 -0.519305  0.005360 -0.008733  0.001372 -0.251839  0.002045   \n",
       "3  0.062337  0.553817  0.053025 -0.008733 -0.003080  0.018702 -0.002474   \n",
       "4  0.072090  0.519236  0.020156 -0.008733  0.005024  0.297389  0.000779   \n",
       "\n",
       "        f29       f30       f31       f32       f33       f34       f35  \\\n",
       "0 -0.010773 -0.027652 -0.123439  0.040229  0.109841  0.000775 -0.002725   \n",
       "1 -0.044777 -0.020852  0.433588 -0.145489  0.176984  0.000775 -0.002182   \n",
       "2  0.048360 -0.034847 -0.469892 -0.054636 -0.129261  0.000775 -0.011611   \n",
       "3 -0.010773  0.086458  0.020163  0.200410  0.091943  0.000775  0.010607   \n",
       "4  0.020482  0.186001  0.280844  0.262483  0.256026  0.000775  0.005474   \n",
       "\n",
       "        f36       f37       f38       f39       f40       f41       f42  \\\n",
       "0 -0.215236 -0.173222  0.859616 -0.003327  1.886817  0.450763  0.508958   \n",
       "1  0.030317  0.034727  1.008091 -0.002790  1.810774  0.289262  0.620846   \n",
       "2 -0.194838 -0.143116  0.357578 -0.012107  1.689672 -0.294596  0.774879   \n",
       "3 -0.073015 -0.061893  0.557530  0.009877  1.360255  0.365248 -0.333936   \n",
       "4  0.445244  0.233803  0.536495  0.004789  1.212282  0.253425  0.327412   \n",
       "\n",
       "        f43       f44       f45       f46       f47       f48       f49  \\\n",
       "0  1.094065 -0.001936  0.065516  0.386516  0.049406  0.002729  0.490221   \n",
       "1  0.491455  0.137752 -0.020472  0.191353 -0.296367 -0.051450 -0.250389   \n",
       "2  0.741813  0.082078 -0.047917  0.386516 -0.296367  0.002728  0.583518   \n",
       "3  0.307492 -0.008507 -0.056499  0.191353  0.074037 -0.007525  0.490221   \n",
       "4 -0.213843 -0.013135  0.046403 -0.374462  0.049406  0.031352  0.108361   \n",
       "\n",
       "        f50       f51       f52       f53       f54       f55       f56  \\\n",
       "0  0.226400 -0.006899 -0.340249  0.618645  0.040241 -0.003450 -0.003327   \n",
       "1 -0.015683 -0.006899  0.169559 -0.400890  0.179168 -0.002913 -0.002791   \n",
       "2  0.034111 -0.006899 -0.311590  0.430037  0.217412 -0.012249 -0.012108   \n",
       "3  0.226400 -0.006899 -0.142420  0.549689  0.399248  0.009783  0.009877   \n",
       "4  0.665547 -0.006899  0.236702  0.231525 -0.325201  0.004683  0.004789   \n",
       "\n",
       "        f57  runner_id  result  event_id   is1   is2    oos         S  \\\n",
       "0  0.036781     353046       3    287398  True  True  False -0.331537   \n",
       "1  0.213007     352217       5    287398  True  True  False -0.213620   \n",
       "2 -0.173009     342962      13    287398  True  True  False -0.094424   \n",
       "3  0.326784     361432       1    287398  True  True  False -0.256826   \n",
       "4 -0.145831     359420       6    287398  True  True  False -0.156319   \n",
       "\n",
       "          p     log_p  cl_x        cl_0_x        cl_1_x        cl_2_x  cl_3_x  \\\n",
       "0  0.053286 -3.310580     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "1  0.059955 -3.546679     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "2  0.067545 -2.829194     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "3  0.057420 -3.334780     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "4  0.063491 -2.936739     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "\n",
       "          cl_4_x  cl_5_x    S_cl_0   p_cl__0    S_cl_1   p_cl__1    S_cl_2  \\\n",
       "0  6.907297e-283     1.0 -0.678621  0.032506 -0.667135  0.032394 -0.861541   \n",
       "1  6.907297e-283     1.0 -0.579957  0.035877 -0.805975  0.028195 -0.281796   \n",
       "2  6.907297e-283     1.0 -0.152950  0.054987 -0.368198  0.043681 -0.167563   \n",
       "3  6.907297e-283     1.0 -0.460113  0.040445 -0.512950  0.037794 -0.503278   \n",
       "4  6.907297e-283     1.0 -0.748355  0.030317 -0.013145  0.062300 -0.315810   \n",
       "\n",
       "    p_cl__2    S_cl_3   p_cl__3    S_cl_4   p_cl__4    S_cl_5   p_cl__5  \\\n",
       "0  0.028221 -1.299192  0.011225  0.351216  0.088811 -0.596701  0.036495   \n",
       "1  0.050391 -1.325842  0.010930  1.418878  0.258312 -0.832801  0.028820   \n",
       "2  0.056489 -0.195104  0.033859 -0.124884  0.055169 -0.115315  0.059060   \n",
       "3  0.040380 -1.607062  0.008250  0.510964  0.104194 -0.620902  0.035622   \n",
       "4  0.048706  0.076846  0.044441  0.736712  0.130582 -0.222861  0.053038   \n",
       "\n",
       "      p_mix  cl_y        cl_0_y  cl_1_y        cl_2_y         cl_3_y  cl_4_y  \\\n",
       "0  0.036495     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "1  0.028820     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "2  0.059060     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "3  0.035622     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "4  0.053038     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "\n",
       "         cl_5_y  \n",
       "0  3.840932e-10  \n",
       "1  3.840932e-10  \n",
       "2  3.840932e-10  \n",
       "3  3.840932e-10  \n",
       "4  3.840932e-10  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>f32</th>\n",
       "      <th>f33</th>\n",
       "      <th>f34</th>\n",
       "      <th>f35</th>\n",
       "      <th>f36</th>\n",
       "      <th>f37</th>\n",
       "      <th>f38</th>\n",
       "      <th>f39</th>\n",
       "      <th>f40</th>\n",
       "      <th>f41</th>\n",
       "      <th>f42</th>\n",
       "      <th>f43</th>\n",
       "      <th>f44</th>\n",
       "      <th>f45</th>\n",
       "      <th>f46</th>\n",
       "      <th>f47</th>\n",
       "      <th>f48</th>\n",
       "      <th>f49</th>\n",
       "      <th>f50</th>\n",
       "      <th>f51</th>\n",
       "      <th>f52</th>\n",
       "      <th>f53</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>runner_id</th>\n",
       "      <th>result</th>\n",
       "      <th>event_id</th>\n",
       "      <th>is1</th>\n",
       "      <th>is2</th>\n",
       "      <th>oos</th>\n",
       "      <th>S</th>\n",
       "      <th>p</th>\n",
       "      <th>log_p</th>\n",
       "      <th>cl_x</th>\n",
       "      <th>cl_0_x</th>\n",
       "      <th>cl_1_x</th>\n",
       "      <th>cl_2_x</th>\n",
       "      <th>cl_3_x</th>\n",
       "      <th>cl_4_x</th>\n",
       "      <th>cl_5_x</th>\n",
       "      <th>S_cl_0</th>\n",
       "      <th>p_cl__0</th>\n",
       "      <th>S_cl_1</th>\n",
       "      <th>p_cl__1</th>\n",
       "      <th>S_cl_2</th>\n",
       "      <th>p_cl__2</th>\n",
       "      <th>S_cl_3</th>\n",
       "      <th>p_cl__3</th>\n",
       "      <th>S_cl_4</th>\n",
       "      <th>p_cl__4</th>\n",
       "      <th>S_cl_5</th>\n",
       "      <th>p_cl__5</th>\n",
       "      <th>p_mix</th>\n",
       "      <th>cl_y</th>\n",
       "      <th>cl_0_y</th>\n",
       "      <th>cl_1_y</th>\n",
       "      <th>cl_2_y</th>\n",
       "      <th>cl_3_y</th>\n",
       "      <th>cl_4_y</th>\n",
       "      <th>cl_5_y</th>\n",
       "      <th>cl</th>\n",
       "      <th>cl_0</th>\n",
       "      <th>cl_1</th>\n",
       "      <th>cl_2</th>\n",
       "      <th>cl_3</th>\n",
       "      <th>cl_4</th>\n",
       "      <th>cl_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.152068</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.971937</td>\n",
       "      <td>1.096287</td>\n",
       "      <td>-0.003309</td>\n",
       "      <td>1.761018</td>\n",
       "      <td>0.968297</td>\n",
       "      <td>0.374753</td>\n",
       "      <td>0.675330</td>\n",
       "      <td>0.051377</td>\n",
       "      <td>-0.005242</td>\n",
       "      <td>1.305105</td>\n",
       "      <td>0.083228</td>\n",
       "      <td>-0.003275</td>\n",
       "      <td>0.032186</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>0.041683</td>\n",
       "      <td>0.015889</td>\n",
       "      <td>-0.358249</td>\n",
       "      <td>0.287939</td>\n",
       "      <td>0.037986</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>-0.001490</td>\n",
       "      <td>-0.472996</td>\n",
       "      <td>-0.003019</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>-0.027652</td>\n",
       "      <td>-0.123439</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.109841</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>-0.002725</td>\n",
       "      <td>-0.215236</td>\n",
       "      <td>-0.173222</td>\n",
       "      <td>0.859616</td>\n",
       "      <td>-0.003327</td>\n",
       "      <td>1.886817</td>\n",
       "      <td>0.450763</td>\n",
       "      <td>0.508958</td>\n",
       "      <td>1.094065</td>\n",
       "      <td>-0.001936</td>\n",
       "      <td>0.065516</td>\n",
       "      <td>0.386516</td>\n",
       "      <td>0.049406</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>0.490221</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>-0.340249</td>\n",
       "      <td>0.618645</td>\n",
       "      <td>0.040241</td>\n",
       "      <td>-0.003450</td>\n",
       "      <td>-0.003327</td>\n",
       "      <td>0.036781</td>\n",
       "      <td>353046</td>\n",
       "      <td>3</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.331537</td>\n",
       "      <td>0.053286</td>\n",
       "      <td>-3.310580</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.678621</td>\n",
       "      <td>0.032506</td>\n",
       "      <td>-0.667135</td>\n",
       "      <td>0.032394</td>\n",
       "      <td>-0.861541</td>\n",
       "      <td>0.028221</td>\n",
       "      <td>-1.299192</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.351216</td>\n",
       "      <td>0.088811</td>\n",
       "      <td>-0.596701</td>\n",
       "      <td>0.036495</td>\n",
       "      <td>0.036495</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.137530</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.398019</td>\n",
       "      <td>0.481980</td>\n",
       "      <td>-0.002772</td>\n",
       "      <td>1.682896</td>\n",
       "      <td>0.354919</td>\n",
       "      <td>-0.994794</td>\n",
       "      <td>0.212441</td>\n",
       "      <td>0.484803</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>1.408825</td>\n",
       "      <td>0.066311</td>\n",
       "      <td>-0.002738</td>\n",
       "      <td>-0.017447</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>-0.012072</td>\n",
       "      <td>-0.023179</td>\n",
       "      <td>0.047898</td>\n",
       "      <td>-0.533036</td>\n",
       "      <td>-0.086651</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.075571</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>-0.044777</td>\n",
       "      <td>-0.020852</td>\n",
       "      <td>0.433588</td>\n",
       "      <td>-0.145489</td>\n",
       "      <td>0.176984</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>-0.002182</td>\n",
       "      <td>0.030317</td>\n",
       "      <td>0.034727</td>\n",
       "      <td>1.008091</td>\n",
       "      <td>-0.002790</td>\n",
       "      <td>1.810774</td>\n",
       "      <td>0.289262</td>\n",
       "      <td>0.620846</td>\n",
       "      <td>0.491455</td>\n",
       "      <td>0.137752</td>\n",
       "      <td>-0.020472</td>\n",
       "      <td>0.191353</td>\n",
       "      <td>-0.296367</td>\n",
       "      <td>-0.051450</td>\n",
       "      <td>-0.250389</td>\n",
       "      <td>-0.015683</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>-0.400890</td>\n",
       "      <td>0.179168</td>\n",
       "      <td>-0.002913</td>\n",
       "      <td>-0.002791</td>\n",
       "      <td>0.213007</td>\n",
       "      <td>352217</td>\n",
       "      <td>5</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.213620</td>\n",
       "      <td>0.059955</td>\n",
       "      <td>-3.546679</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.579957</td>\n",
       "      <td>0.035877</td>\n",
       "      <td>-0.805975</td>\n",
       "      <td>0.028195</td>\n",
       "      <td>-0.281796</td>\n",
       "      <td>0.050391</td>\n",
       "      <td>-1.325842</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>1.418878</td>\n",
       "      <td>0.258312</td>\n",
       "      <td>-0.832801</td>\n",
       "      <td>0.028820</td>\n",
       "      <td>0.028820</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.029570</td>\n",
       "      <td>-0.145221</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.307072</td>\n",
       "      <td>0.427015</td>\n",
       "      <td>-0.012089</td>\n",
       "      <td>1.165470</td>\n",
       "      <td>0.440719</td>\n",
       "      <td>0.135928</td>\n",
       "      <td>0.519989</td>\n",
       "      <td>0.158056</td>\n",
       "      <td>0.013730</td>\n",
       "      <td>0.722187</td>\n",
       "      <td>0.170334</td>\n",
       "      <td>-0.012058</td>\n",
       "      <td>-0.011141</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>-0.013402</td>\n",
       "      <td>-0.022607</td>\n",
       "      <td>-0.209299</td>\n",
       "      <td>-0.519305</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>-0.251839</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.048360</td>\n",
       "      <td>-0.034847</td>\n",
       "      <td>-0.469892</td>\n",
       "      <td>-0.054636</td>\n",
       "      <td>-0.129261</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>-0.011611</td>\n",
       "      <td>-0.194838</td>\n",
       "      <td>-0.143116</td>\n",
       "      <td>0.357578</td>\n",
       "      <td>-0.012107</td>\n",
       "      <td>1.689672</td>\n",
       "      <td>-0.294596</td>\n",
       "      <td>0.774879</td>\n",
       "      <td>0.741813</td>\n",
       "      <td>0.082078</td>\n",
       "      <td>-0.047917</td>\n",
       "      <td>0.386516</td>\n",
       "      <td>-0.296367</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.583518</td>\n",
       "      <td>0.034111</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>-0.311590</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.217412</td>\n",
       "      <td>-0.012249</td>\n",
       "      <td>-0.012108</td>\n",
       "      <td>-0.173009</td>\n",
       "      <td>342962</td>\n",
       "      <td>13</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.094424</td>\n",
       "      <td>0.067545</td>\n",
       "      <td>-2.829194</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.152950</td>\n",
       "      <td>0.054987</td>\n",
       "      <td>-0.368198</td>\n",
       "      <td>0.043681</td>\n",
       "      <td>-0.167563</td>\n",
       "      <td>0.056489</td>\n",
       "      <td>-0.195104</td>\n",
       "      <td>0.033859</td>\n",
       "      <td>-0.124884</td>\n",
       "      <td>0.055169</td>\n",
       "      <td>-0.115315</td>\n",
       "      <td>0.059060</td>\n",
       "      <td>0.059060</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.136884</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.234280</td>\n",
       "      <td>0.361007</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>0.880806</td>\n",
       "      <td>1.230350</td>\n",
       "      <td>0.374753</td>\n",
       "      <td>0.212441</td>\n",
       "      <td>0.584892</td>\n",
       "      <td>0.025233</td>\n",
       "      <td>1.098121</td>\n",
       "      <td>0.119076</td>\n",
       "      <td>0.009932</td>\n",
       "      <td>0.020942</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>-0.012072</td>\n",
       "      <td>0.015889</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>0.553817</td>\n",
       "      <td>0.053025</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>-0.003080</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>-0.002474</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>0.086458</td>\n",
       "      <td>0.020163</td>\n",
       "      <td>0.200410</td>\n",
       "      <td>0.091943</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>-0.073015</td>\n",
       "      <td>-0.061893</td>\n",
       "      <td>0.557530</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>1.360255</td>\n",
       "      <td>0.365248</td>\n",
       "      <td>-0.333936</td>\n",
       "      <td>0.307492</td>\n",
       "      <td>-0.008507</td>\n",
       "      <td>-0.056499</td>\n",
       "      <td>0.191353</td>\n",
       "      <td>0.074037</td>\n",
       "      <td>-0.007525</td>\n",
       "      <td>0.490221</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>-0.142420</td>\n",
       "      <td>0.549689</td>\n",
       "      <td>0.399248</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.326784</td>\n",
       "      <td>361432</td>\n",
       "      <td>1</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.256826</td>\n",
       "      <td>0.057420</td>\n",
       "      <td>-3.334780</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.460113</td>\n",
       "      <td>0.040445</td>\n",
       "      <td>-0.512950</td>\n",
       "      <td>0.037794</td>\n",
       "      <td>-0.503278</td>\n",
       "      <td>0.040380</td>\n",
       "      <td>-1.607062</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.510964</td>\n",
       "      <td>0.104194</td>\n",
       "      <td>-0.620902</td>\n",
       "      <td>0.035622</td>\n",
       "      <td>0.035622</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.343377</td>\n",
       "      <td>0.242782</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.017472</td>\n",
       "      <td>-0.199782</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.481565</td>\n",
       "      <td>1.404777</td>\n",
       "      <td>0.326130</td>\n",
       "      <td>0.248326</td>\n",
       "      <td>-0.371251</td>\n",
       "      <td>-0.005610</td>\n",
       "      <td>1.051047</td>\n",
       "      <td>-0.331326</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.043404</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>0.009731</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.072090</td>\n",
       "      <td>0.519236</td>\n",
       "      <td>0.020156</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.297389</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.020482</td>\n",
       "      <td>0.186001</td>\n",
       "      <td>0.280844</td>\n",
       "      <td>0.262483</td>\n",
       "      <td>0.256026</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.005474</td>\n",
       "      <td>0.445244</td>\n",
       "      <td>0.233803</td>\n",
       "      <td>0.536495</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>1.212282</td>\n",
       "      <td>0.253425</td>\n",
       "      <td>0.327412</td>\n",
       "      <td>-0.213843</td>\n",
       "      <td>-0.013135</td>\n",
       "      <td>0.046403</td>\n",
       "      <td>-0.374462</td>\n",
       "      <td>0.049406</td>\n",
       "      <td>0.031352</td>\n",
       "      <td>0.108361</td>\n",
       "      <td>0.665547</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>0.236702</td>\n",
       "      <td>0.231525</td>\n",
       "      <td>-0.325201</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>-0.145831</td>\n",
       "      <td>359420</td>\n",
       "      <td>6</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.156319</td>\n",
       "      <td>0.063491</td>\n",
       "      <td>-2.936739</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.748355</td>\n",
       "      <td>0.030317</td>\n",
       "      <td>-0.013145</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>-0.315810</td>\n",
       "      <td>0.048706</td>\n",
       "      <td>0.076846</td>\n",
       "      <td>0.044441</td>\n",
       "      <td>0.736712</td>\n",
       "      <td>0.130582</td>\n",
       "      <td>-0.222861</td>\n",
       "      <td>0.053038</td>\n",
       "      <td>0.053038</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2        f3        f4      f5        f6        f7  \\\n",
       "0  0.064913  1.395654 -0.152068  0.008172  0.0007  0.971937  1.096287   \n",
       "1 -0.250759  1.137530  0.019747  0.008894  0.0007  0.398019  0.481980   \n",
       "2 -0.250759  1.029570 -0.145221 -0.020776  0.0007  0.307072  0.427015   \n",
       "3  0.064913  1.395654 -0.136884 -0.020776  0.0007  0.234280  0.361007   \n",
       "4  0.064913  1.343377  0.242782  0.008894  0.0007 -0.017472 -0.199782   \n",
       "\n",
       "         f8        f9       f10       f11       f12       f13       f14  \\\n",
       "0 -0.003309  1.761018  0.968297  0.374753  0.675330  0.051377 -0.005242   \n",
       "1 -0.002772  1.682896  0.354919 -0.994794  0.212441  0.484803  0.010791   \n",
       "2 -0.012089  1.165470  0.440719  0.135928  0.519989  0.158056  0.013730   \n",
       "3  0.009897  0.880806  1.230350  0.374753  0.212441  0.584892  0.025233   \n",
       "4  0.004808  0.481565  1.404777  0.326130  0.248326 -0.371251 -0.005610   \n",
       "\n",
       "        f15       f16       f17       f18      f19       f20       f21  \\\n",
       "0  1.305105  0.083228 -0.003275  0.032186  0.00289  0.041683  0.015889   \n",
       "1  1.408825  0.066311 -0.002738 -0.017447  0.00289 -0.012072 -0.023179   \n",
       "2  0.722187  0.170334 -0.012058 -0.011141  0.00289 -0.013402 -0.022607   \n",
       "3  1.098121  0.119076  0.009932  0.020942  0.00289 -0.012072  0.015889   \n",
       "4  1.051047 -0.331326  0.004842  0.043404  0.00289  0.009731  0.004371   \n",
       "\n",
       "        f22       f23       f24       f25       f26       f27       f28  \\\n",
       "0 -0.358249  0.287939  0.037986 -0.008733 -0.001490 -0.472996 -0.003019   \n",
       "1  0.047898 -0.533036 -0.086651 -0.008733  0.000512  0.075571  0.000948   \n",
       "2 -0.209299 -0.519305  0.005360 -0.008733  0.001372 -0.251839  0.002045   \n",
       "3  0.062337  0.553817  0.053025 -0.008733 -0.003080  0.018702 -0.002474   \n",
       "4  0.072090  0.519236  0.020156 -0.008733  0.005024  0.297389  0.000779   \n",
       "\n",
       "        f29       f30       f31       f32       f33       f34       f35  \\\n",
       "0 -0.010773 -0.027652 -0.123439  0.040229  0.109841  0.000775 -0.002725   \n",
       "1 -0.044777 -0.020852  0.433588 -0.145489  0.176984  0.000775 -0.002182   \n",
       "2  0.048360 -0.034847 -0.469892 -0.054636 -0.129261  0.000775 -0.011611   \n",
       "3 -0.010773  0.086458  0.020163  0.200410  0.091943  0.000775  0.010607   \n",
       "4  0.020482  0.186001  0.280844  0.262483  0.256026  0.000775  0.005474   \n",
       "\n",
       "        f36       f37       f38       f39       f40       f41       f42  \\\n",
       "0 -0.215236 -0.173222  0.859616 -0.003327  1.886817  0.450763  0.508958   \n",
       "1  0.030317  0.034727  1.008091 -0.002790  1.810774  0.289262  0.620846   \n",
       "2 -0.194838 -0.143116  0.357578 -0.012107  1.689672 -0.294596  0.774879   \n",
       "3 -0.073015 -0.061893  0.557530  0.009877  1.360255  0.365248 -0.333936   \n",
       "4  0.445244  0.233803  0.536495  0.004789  1.212282  0.253425  0.327412   \n",
       "\n",
       "        f43       f44       f45       f46       f47       f48       f49  \\\n",
       "0  1.094065 -0.001936  0.065516  0.386516  0.049406  0.002729  0.490221   \n",
       "1  0.491455  0.137752 -0.020472  0.191353 -0.296367 -0.051450 -0.250389   \n",
       "2  0.741813  0.082078 -0.047917  0.386516 -0.296367  0.002728  0.583518   \n",
       "3  0.307492 -0.008507 -0.056499  0.191353  0.074037 -0.007525  0.490221   \n",
       "4 -0.213843 -0.013135  0.046403 -0.374462  0.049406  0.031352  0.108361   \n",
       "\n",
       "        f50       f51       f52       f53       f54       f55       f56  \\\n",
       "0  0.226400 -0.006899 -0.340249  0.618645  0.040241 -0.003450 -0.003327   \n",
       "1 -0.015683 -0.006899  0.169559 -0.400890  0.179168 -0.002913 -0.002791   \n",
       "2  0.034111 -0.006899 -0.311590  0.430037  0.217412 -0.012249 -0.012108   \n",
       "3  0.226400 -0.006899 -0.142420  0.549689  0.399248  0.009783  0.009877   \n",
       "4  0.665547 -0.006899  0.236702  0.231525 -0.325201  0.004683  0.004789   \n",
       "\n",
       "        f57  runner_id  result  event_id   is1   is2    oos         S  \\\n",
       "0  0.036781     353046       3    287398  True  True  False -0.331537   \n",
       "1  0.213007     352217       5    287398  True  True  False -0.213620   \n",
       "2 -0.173009     342962      13    287398  True  True  False -0.094424   \n",
       "3  0.326784     361432       1    287398  True  True  False -0.256826   \n",
       "4 -0.145831     359420       6    287398  True  True  False -0.156319   \n",
       "\n",
       "          p     log_p  cl_x        cl_0_x        cl_1_x        cl_2_x  cl_3_x  \\\n",
       "0  0.053286 -3.310580     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "1  0.059955 -3.546679     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "2  0.067545 -2.829194     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "3  0.057420 -3.334780     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "4  0.063491 -2.936739     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "\n",
       "          cl_4_x  cl_5_x    S_cl_0   p_cl__0    S_cl_1   p_cl__1    S_cl_2  \\\n",
       "0  6.907297e-283     1.0 -0.678621  0.032506 -0.667135  0.032394 -0.861541   \n",
       "1  6.907297e-283     1.0 -0.579957  0.035877 -0.805975  0.028195 -0.281796   \n",
       "2  6.907297e-283     1.0 -0.152950  0.054987 -0.368198  0.043681 -0.167563   \n",
       "3  6.907297e-283     1.0 -0.460113  0.040445 -0.512950  0.037794 -0.503278   \n",
       "4  6.907297e-283     1.0 -0.748355  0.030317 -0.013145  0.062300 -0.315810   \n",
       "\n",
       "    p_cl__2    S_cl_3   p_cl__3    S_cl_4   p_cl__4    S_cl_5   p_cl__5  \\\n",
       "0  0.028221 -1.299192  0.011225  0.351216  0.088811 -0.596701  0.036495   \n",
       "1  0.050391 -1.325842  0.010930  1.418878  0.258312 -0.832801  0.028820   \n",
       "2  0.056489 -0.195104  0.033859 -0.124884  0.055169 -0.115315  0.059060   \n",
       "3  0.040380 -1.607062  0.008250  0.510964  0.104194 -0.620902  0.035622   \n",
       "4  0.048706  0.076846  0.044441  0.736712  0.130582 -0.222861  0.053038   \n",
       "\n",
       "      p_mix  cl_y        cl_0_y  cl_1_y        cl_2_y         cl_3_y  cl_4_y  \\\n",
       "0  0.036495     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "1  0.028820     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "2  0.059060     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "3  0.035622     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "4  0.053038     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "\n",
       "         cl_5_y  cl          cl_0  cl_1          cl_2           cl_3  cl_4  \\\n",
       "0  3.840932e-10   1  1.552685e-99   1.0  3.280995e-09  4.502352e-271   0.0   \n",
       "1  3.840932e-10   1  1.552685e-99   1.0  3.280995e-09  4.502352e-271   0.0   \n",
       "2  3.840932e-10   1  1.552685e-99   1.0  3.280995e-09  4.502352e-271   0.0   \n",
       "3  3.840932e-10   1  1.552685e-99   1.0  3.280995e-09  4.502352e-271   0.0   \n",
       "4  3.840932e-10   1  1.552685e-99   1.0  3.280995e-09  4.502352e-271   0.0   \n",
       "\n",
       "           cl_5  \n",
       "0  3.840932e-10  \n",
       "1  3.840932e-10  \n",
       "2  3.840932e-10  \n",
       "3  3.840932e-10  \n",
       "4  3.840932e-10  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.merge(df_cluster_add, left_on = df['event_id'], right_index = True)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>f32</th>\n",
       "      <th>f33</th>\n",
       "      <th>f34</th>\n",
       "      <th>f35</th>\n",
       "      <th>f36</th>\n",
       "      <th>f37</th>\n",
       "      <th>f38</th>\n",
       "      <th>f39</th>\n",
       "      <th>f40</th>\n",
       "      <th>f41</th>\n",
       "      <th>f42</th>\n",
       "      <th>f43</th>\n",
       "      <th>f44</th>\n",
       "      <th>f45</th>\n",
       "      <th>f46</th>\n",
       "      <th>f47</th>\n",
       "      <th>f48</th>\n",
       "      <th>f49</th>\n",
       "      <th>f50</th>\n",
       "      <th>f51</th>\n",
       "      <th>f52</th>\n",
       "      <th>f53</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>runner_id</th>\n",
       "      <th>result</th>\n",
       "      <th>event_id</th>\n",
       "      <th>is1</th>\n",
       "      <th>is2</th>\n",
       "      <th>oos</th>\n",
       "      <th>S</th>\n",
       "      <th>p</th>\n",
       "      <th>log_p</th>\n",
       "      <th>cl_x</th>\n",
       "      <th>cl_0_x</th>\n",
       "      <th>cl_1_x</th>\n",
       "      <th>cl_2_x</th>\n",
       "      <th>cl_3_x</th>\n",
       "      <th>cl_4_x</th>\n",
       "      <th>cl_5_x</th>\n",
       "      <th>S_cl_0</th>\n",
       "      <th>p_cl__0</th>\n",
       "      <th>S_cl_1</th>\n",
       "      <th>p_cl__1</th>\n",
       "      <th>S_cl_2</th>\n",
       "      <th>p_cl__2</th>\n",
       "      <th>S_cl_3</th>\n",
       "      <th>p_cl__3</th>\n",
       "      <th>S_cl_4</th>\n",
       "      <th>p_cl__4</th>\n",
       "      <th>S_cl_5</th>\n",
       "      <th>p_cl__5</th>\n",
       "      <th>p_mix</th>\n",
       "      <th>cl_y</th>\n",
       "      <th>cl_0_y</th>\n",
       "      <th>cl_1_y</th>\n",
       "      <th>cl_2_y</th>\n",
       "      <th>cl_3_y</th>\n",
       "      <th>cl_4_y</th>\n",
       "      <th>cl_5_y</th>\n",
       "      <th>cl</th>\n",
       "      <th>cl_0</th>\n",
       "      <th>cl_1</th>\n",
       "      <th>cl_2</th>\n",
       "      <th>cl_3</th>\n",
       "      <th>cl_4</th>\n",
       "      <th>cl_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.152068</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.971937</td>\n",
       "      <td>1.096287</td>\n",
       "      <td>-0.003309</td>\n",
       "      <td>1.761018</td>\n",
       "      <td>0.968297</td>\n",
       "      <td>0.374753</td>\n",
       "      <td>0.675330</td>\n",
       "      <td>0.051377</td>\n",
       "      <td>-0.005242</td>\n",
       "      <td>1.305105</td>\n",
       "      <td>0.083228</td>\n",
       "      <td>-0.003275</td>\n",
       "      <td>0.032186</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>0.041683</td>\n",
       "      <td>0.015889</td>\n",
       "      <td>-0.358249</td>\n",
       "      <td>0.287939</td>\n",
       "      <td>0.037986</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>-0.001490</td>\n",
       "      <td>-0.472996</td>\n",
       "      <td>-0.003019</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>-0.027652</td>\n",
       "      <td>-0.123439</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.109841</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>-0.002725</td>\n",
       "      <td>-0.215236</td>\n",
       "      <td>-0.173222</td>\n",
       "      <td>0.859616</td>\n",
       "      <td>-0.003327</td>\n",
       "      <td>1.886817</td>\n",
       "      <td>0.450763</td>\n",
       "      <td>0.508958</td>\n",
       "      <td>1.094065</td>\n",
       "      <td>-0.001936</td>\n",
       "      <td>0.065516</td>\n",
       "      <td>0.386516</td>\n",
       "      <td>0.049406</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>0.490221</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>-0.340249</td>\n",
       "      <td>0.618645</td>\n",
       "      <td>0.040241</td>\n",
       "      <td>-0.003450</td>\n",
       "      <td>-0.003327</td>\n",
       "      <td>0.036781</td>\n",
       "      <td>353046</td>\n",
       "      <td>3</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.331537</td>\n",
       "      <td>0.053286</td>\n",
       "      <td>-3.310580</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.766702</td>\n",
       "      <td>0.022774</td>\n",
       "      <td>-0.864577</td>\n",
       "      <td>0.024444</td>\n",
       "      <td>-0.709182</td>\n",
       "      <td>0.031982</td>\n",
       "      <td>-1.164725</td>\n",
       "      <td>0.013549</td>\n",
       "      <td>-2.072146</td>\n",
       "      <td>0.005716</td>\n",
       "      <td>-0.615573</td>\n",
       "      <td>0.032584</td>\n",
       "      <td>0.036495</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.137530</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.398019</td>\n",
       "      <td>0.481980</td>\n",
       "      <td>-0.002772</td>\n",
       "      <td>1.682896</td>\n",
       "      <td>0.354919</td>\n",
       "      <td>-0.994794</td>\n",
       "      <td>0.212441</td>\n",
       "      <td>0.484803</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>1.408825</td>\n",
       "      <td>0.066311</td>\n",
       "      <td>-0.002738</td>\n",
       "      <td>-0.017447</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>-0.012072</td>\n",
       "      <td>-0.023179</td>\n",
       "      <td>0.047898</td>\n",
       "      <td>-0.533036</td>\n",
       "      <td>-0.086651</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.075571</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>-0.044777</td>\n",
       "      <td>-0.020852</td>\n",
       "      <td>0.433588</td>\n",
       "      <td>-0.145489</td>\n",
       "      <td>0.176984</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>-0.002182</td>\n",
       "      <td>0.030317</td>\n",
       "      <td>0.034727</td>\n",
       "      <td>1.008091</td>\n",
       "      <td>-0.002790</td>\n",
       "      <td>1.810774</td>\n",
       "      <td>0.289262</td>\n",
       "      <td>0.620846</td>\n",
       "      <td>0.491455</td>\n",
       "      <td>0.137752</td>\n",
       "      <td>-0.020472</td>\n",
       "      <td>0.191353</td>\n",
       "      <td>-0.296367</td>\n",
       "      <td>-0.051450</td>\n",
       "      <td>-0.250389</td>\n",
       "      <td>-0.015683</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>-0.400890</td>\n",
       "      <td>0.179168</td>\n",
       "      <td>-0.002913</td>\n",
       "      <td>-0.002791</td>\n",
       "      <td>0.213007</td>\n",
       "      <td>352217</td>\n",
       "      <td>5</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.213620</td>\n",
       "      <td>0.059955</td>\n",
       "      <td>-3.546679</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.714238</td>\n",
       "      <td>0.024001</td>\n",
       "      <td>-0.536705</td>\n",
       "      <td>0.033928</td>\n",
       "      <td>-0.275155</td>\n",
       "      <td>0.049363</td>\n",
       "      <td>0.008524</td>\n",
       "      <td>0.043798</td>\n",
       "      <td>-1.809029</td>\n",
       "      <td>0.007437</td>\n",
       "      <td>-1.079423</td>\n",
       "      <td>0.020491</td>\n",
       "      <td>0.028820</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.029570</td>\n",
       "      <td>-0.145221</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.307072</td>\n",
       "      <td>0.427015</td>\n",
       "      <td>-0.012089</td>\n",
       "      <td>1.165470</td>\n",
       "      <td>0.440719</td>\n",
       "      <td>0.135928</td>\n",
       "      <td>0.519989</td>\n",
       "      <td>0.158056</td>\n",
       "      <td>0.013730</td>\n",
       "      <td>0.722187</td>\n",
       "      <td>0.170334</td>\n",
       "      <td>-0.012058</td>\n",
       "      <td>-0.011141</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>-0.013402</td>\n",
       "      <td>-0.022607</td>\n",
       "      <td>-0.209299</td>\n",
       "      <td>-0.519305</td>\n",
       "      <td>0.005360</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>-0.251839</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.048360</td>\n",
       "      <td>-0.034847</td>\n",
       "      <td>-0.469892</td>\n",
       "      <td>-0.054636</td>\n",
       "      <td>-0.129261</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>-0.011611</td>\n",
       "      <td>-0.194838</td>\n",
       "      <td>-0.143116</td>\n",
       "      <td>0.357578</td>\n",
       "      <td>-0.012107</td>\n",
       "      <td>1.689672</td>\n",
       "      <td>-0.294596</td>\n",
       "      <td>0.774879</td>\n",
       "      <td>0.741813</td>\n",
       "      <td>0.082078</td>\n",
       "      <td>-0.047917</td>\n",
       "      <td>0.386516</td>\n",
       "      <td>-0.296367</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.583518</td>\n",
       "      <td>0.034111</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>-0.311590</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.217412</td>\n",
       "      <td>-0.012249</td>\n",
       "      <td>-0.012108</td>\n",
       "      <td>-0.173009</td>\n",
       "      <td>342962</td>\n",
       "      <td>13</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.094424</td>\n",
       "      <td>0.067545</td>\n",
       "      <td>-2.829194</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.262025</td>\n",
       "      <td>0.037724</td>\n",
       "      <td>-0.273273</td>\n",
       "      <td>0.044154</td>\n",
       "      <td>-0.016487</td>\n",
       "      <td>0.063935</td>\n",
       "      <td>-0.810425</td>\n",
       "      <td>0.019310</td>\n",
       "      <td>-1.342198</td>\n",
       "      <td>0.011861</td>\n",
       "      <td>-0.488618</td>\n",
       "      <td>0.036995</td>\n",
       "      <td>0.059060</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.136884</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.234280</td>\n",
       "      <td>0.361007</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>0.880806</td>\n",
       "      <td>1.230350</td>\n",
       "      <td>0.374753</td>\n",
       "      <td>0.212441</td>\n",
       "      <td>0.584892</td>\n",
       "      <td>0.025233</td>\n",
       "      <td>1.098121</td>\n",
       "      <td>0.119076</td>\n",
       "      <td>0.009932</td>\n",
       "      <td>0.020942</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>-0.012072</td>\n",
       "      <td>0.015889</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>0.553817</td>\n",
       "      <td>0.053025</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>-0.003080</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>-0.002474</td>\n",
       "      <td>-0.010773</td>\n",
       "      <td>0.086458</td>\n",
       "      <td>0.020163</td>\n",
       "      <td>0.200410</td>\n",
       "      <td>0.091943</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>-0.073015</td>\n",
       "      <td>-0.061893</td>\n",
       "      <td>0.557530</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>1.360255</td>\n",
       "      <td>0.365248</td>\n",
       "      <td>-0.333936</td>\n",
       "      <td>0.307492</td>\n",
       "      <td>-0.008507</td>\n",
       "      <td>-0.056499</td>\n",
       "      <td>0.191353</td>\n",
       "      <td>0.074037</td>\n",
       "      <td>-0.007525</td>\n",
       "      <td>0.490221</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>-0.142420</td>\n",
       "      <td>0.549689</td>\n",
       "      <td>0.399248</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.326784</td>\n",
       "      <td>361432</td>\n",
       "      <td>1</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.256826</td>\n",
       "      <td>0.057420</td>\n",
       "      <td>-3.334780</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.652312</td>\n",
       "      <td>0.025534</td>\n",
       "      <td>-0.721107</td>\n",
       "      <td>0.028215</td>\n",
       "      <td>-0.435250</td>\n",
       "      <td>0.042060</td>\n",
       "      <td>-1.040604</td>\n",
       "      <td>0.015340</td>\n",
       "      <td>-0.852562</td>\n",
       "      <td>0.019354</td>\n",
       "      <td>-0.442834</td>\n",
       "      <td>0.038728</td>\n",
       "      <td>0.035622</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.343377</td>\n",
       "      <td>0.242782</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.017472</td>\n",
       "      <td>-0.199782</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.481565</td>\n",
       "      <td>1.404777</td>\n",
       "      <td>0.326130</td>\n",
       "      <td>0.248326</td>\n",
       "      <td>-0.371251</td>\n",
       "      <td>-0.005610</td>\n",
       "      <td>1.051047</td>\n",
       "      <td>-0.331326</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.043404</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>0.009731</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.072090</td>\n",
       "      <td>0.519236</td>\n",
       "      <td>0.020156</td>\n",
       "      <td>-0.008733</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.297389</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.020482</td>\n",
       "      <td>0.186001</td>\n",
       "      <td>0.280844</td>\n",
       "      <td>0.262483</td>\n",
       "      <td>0.256026</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.005474</td>\n",
       "      <td>0.445244</td>\n",
       "      <td>0.233803</td>\n",
       "      <td>0.536495</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>1.212282</td>\n",
       "      <td>0.253425</td>\n",
       "      <td>0.327412</td>\n",
       "      <td>-0.213843</td>\n",
       "      <td>-0.013135</td>\n",
       "      <td>0.046403</td>\n",
       "      <td>-0.374462</td>\n",
       "      <td>0.049406</td>\n",
       "      <td>0.031352</td>\n",
       "      <td>0.108361</td>\n",
       "      <td>0.665547</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>0.236702</td>\n",
       "      <td>0.231525</td>\n",
       "      <td>-0.325201</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>-0.145831</td>\n",
       "      <td>359420</td>\n",
       "      <td>6</td>\n",
       "      <td>287398</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.156319</td>\n",
       "      <td>0.063491</td>\n",
       "      <td>-2.936739</td>\n",
       "      <td>5</td>\n",
       "      <td>2.583198e-76</td>\n",
       "      <td>1.217674e-16</td>\n",
       "      <td>1.656811e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.907297e-283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.976276</td>\n",
       "      <td>0.018468</td>\n",
       "      <td>-0.355679</td>\n",
       "      <td>0.040661</td>\n",
       "      <td>-0.319973</td>\n",
       "      <td>0.047199</td>\n",
       "      <td>-0.738494</td>\n",
       "      <td>0.020750</td>\n",
       "      <td>-0.031899</td>\n",
       "      <td>0.043971</td>\n",
       "      <td>-0.193393</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.053038</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.552685e-99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.280995e-09</td>\n",
       "      <td>4.502352e-271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.840932e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2        f3        f4      f5        f6        f7  \\\n",
       "0  0.064913  1.395654 -0.152068  0.008172  0.0007  0.971937  1.096287   \n",
       "1 -0.250759  1.137530  0.019747  0.008894  0.0007  0.398019  0.481980   \n",
       "2 -0.250759  1.029570 -0.145221 -0.020776  0.0007  0.307072  0.427015   \n",
       "3  0.064913  1.395654 -0.136884 -0.020776  0.0007  0.234280  0.361007   \n",
       "4  0.064913  1.343377  0.242782  0.008894  0.0007 -0.017472 -0.199782   \n",
       "\n",
       "         f8        f9       f10       f11       f12       f13       f14  \\\n",
       "0 -0.003309  1.761018  0.968297  0.374753  0.675330  0.051377 -0.005242   \n",
       "1 -0.002772  1.682896  0.354919 -0.994794  0.212441  0.484803  0.010791   \n",
       "2 -0.012089  1.165470  0.440719  0.135928  0.519989  0.158056  0.013730   \n",
       "3  0.009897  0.880806  1.230350  0.374753  0.212441  0.584892  0.025233   \n",
       "4  0.004808  0.481565  1.404777  0.326130  0.248326 -0.371251 -0.005610   \n",
       "\n",
       "        f15       f16       f17       f18      f19       f20       f21  \\\n",
       "0  1.305105  0.083228 -0.003275  0.032186  0.00289  0.041683  0.015889   \n",
       "1  1.408825  0.066311 -0.002738 -0.017447  0.00289 -0.012072 -0.023179   \n",
       "2  0.722187  0.170334 -0.012058 -0.011141  0.00289 -0.013402 -0.022607   \n",
       "3  1.098121  0.119076  0.009932  0.020942  0.00289 -0.012072  0.015889   \n",
       "4  1.051047 -0.331326  0.004842  0.043404  0.00289  0.009731  0.004371   \n",
       "\n",
       "        f22       f23       f24       f25       f26       f27       f28  \\\n",
       "0 -0.358249  0.287939  0.037986 -0.008733 -0.001490 -0.472996 -0.003019   \n",
       "1  0.047898 -0.533036 -0.086651 -0.008733  0.000512  0.075571  0.000948   \n",
       "2 -0.209299 -0.519305  0.005360 -0.008733  0.001372 -0.251839  0.002045   \n",
       "3  0.062337  0.553817  0.053025 -0.008733 -0.003080  0.018702 -0.002474   \n",
       "4  0.072090  0.519236  0.020156 -0.008733  0.005024  0.297389  0.000779   \n",
       "\n",
       "        f29       f30       f31       f32       f33       f34       f35  \\\n",
       "0 -0.010773 -0.027652 -0.123439  0.040229  0.109841  0.000775 -0.002725   \n",
       "1 -0.044777 -0.020852  0.433588 -0.145489  0.176984  0.000775 -0.002182   \n",
       "2  0.048360 -0.034847 -0.469892 -0.054636 -0.129261  0.000775 -0.011611   \n",
       "3 -0.010773  0.086458  0.020163  0.200410  0.091943  0.000775  0.010607   \n",
       "4  0.020482  0.186001  0.280844  0.262483  0.256026  0.000775  0.005474   \n",
       "\n",
       "        f36       f37       f38       f39       f40       f41       f42  \\\n",
       "0 -0.215236 -0.173222  0.859616 -0.003327  1.886817  0.450763  0.508958   \n",
       "1  0.030317  0.034727  1.008091 -0.002790  1.810774  0.289262  0.620846   \n",
       "2 -0.194838 -0.143116  0.357578 -0.012107  1.689672 -0.294596  0.774879   \n",
       "3 -0.073015 -0.061893  0.557530  0.009877  1.360255  0.365248 -0.333936   \n",
       "4  0.445244  0.233803  0.536495  0.004789  1.212282  0.253425  0.327412   \n",
       "\n",
       "        f43       f44       f45       f46       f47       f48       f49  \\\n",
       "0  1.094065 -0.001936  0.065516  0.386516  0.049406  0.002729  0.490221   \n",
       "1  0.491455  0.137752 -0.020472  0.191353 -0.296367 -0.051450 -0.250389   \n",
       "2  0.741813  0.082078 -0.047917  0.386516 -0.296367  0.002728  0.583518   \n",
       "3  0.307492 -0.008507 -0.056499  0.191353  0.074037 -0.007525  0.490221   \n",
       "4 -0.213843 -0.013135  0.046403 -0.374462  0.049406  0.031352  0.108361   \n",
       "\n",
       "        f50       f51       f52       f53       f54       f55       f56  \\\n",
       "0  0.226400 -0.006899 -0.340249  0.618645  0.040241 -0.003450 -0.003327   \n",
       "1 -0.015683 -0.006899  0.169559 -0.400890  0.179168 -0.002913 -0.002791   \n",
       "2  0.034111 -0.006899 -0.311590  0.430037  0.217412 -0.012249 -0.012108   \n",
       "3  0.226400 -0.006899 -0.142420  0.549689  0.399248  0.009783  0.009877   \n",
       "4  0.665547 -0.006899  0.236702  0.231525 -0.325201  0.004683  0.004789   \n",
       "\n",
       "        f57  runner_id  result  event_id   is1   is2    oos         S  \\\n",
       "0  0.036781     353046       3    287398  True  True  False -0.331537   \n",
       "1  0.213007     352217       5    287398  True  True  False -0.213620   \n",
       "2 -0.173009     342962      13    287398  True  True  False -0.094424   \n",
       "3  0.326784     361432       1    287398  True  True  False -0.256826   \n",
       "4 -0.145831     359420       6    287398  True  True  False -0.156319   \n",
       "\n",
       "          p     log_p  cl_x        cl_0_x        cl_1_x        cl_2_x  cl_3_x  \\\n",
       "0  0.053286 -3.310580     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "1  0.059955 -3.546679     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "2  0.067545 -2.829194     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "3  0.057420 -3.334780     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "4  0.063491 -2.936739     5  2.583198e-76  1.217674e-16  1.656811e-08     0.0   \n",
       "\n",
       "          cl_4_x  cl_5_x    S_cl_0   p_cl__0    S_cl_1   p_cl__1    S_cl_2  \\\n",
       "0  6.907297e-283     1.0 -0.766702  0.022774 -0.864577  0.024444 -0.709182   \n",
       "1  6.907297e-283     1.0 -0.714238  0.024001 -0.536705  0.033928 -0.275155   \n",
       "2  6.907297e-283     1.0 -0.262025  0.037724 -0.273273  0.044154 -0.016487   \n",
       "3  6.907297e-283     1.0 -0.652312  0.025534 -0.721107  0.028215 -0.435250   \n",
       "4  6.907297e-283     1.0 -0.976276  0.018468 -0.355679  0.040661 -0.319973   \n",
       "\n",
       "    p_cl__2    S_cl_3   p_cl__3    S_cl_4   p_cl__4    S_cl_5   p_cl__5  \\\n",
       "0  0.031982 -1.164725  0.013549 -2.072146  0.005716 -0.615573  0.032584   \n",
       "1  0.049363  0.008524  0.043798 -1.809029  0.007437 -1.079423  0.020491   \n",
       "2  0.063935 -0.810425  0.019310 -1.342198  0.011861 -0.488618  0.036995   \n",
       "3  0.042060 -1.040604  0.015340 -0.852562  0.019354 -0.442834  0.038728   \n",
       "4  0.047199 -0.738494  0.020750 -0.031899  0.043971 -0.193393  0.049700   \n",
       "\n",
       "      p_mix  cl_y        cl_0_y  cl_1_y        cl_2_y         cl_3_y  cl_4_y  \\\n",
       "0  0.036495     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "1  0.028820     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "2  0.059060     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "3  0.035622     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "4  0.053038     1  1.552685e-99     1.0  3.280995e-09  4.502352e-271     0.0   \n",
       "\n",
       "         cl_5_y  cl          cl_0  cl_1          cl_2           cl_3  cl_4  \\\n",
       "0  3.840932e-10   1  1.552685e-99   1.0  3.280995e-09  4.502352e-271   0.0   \n",
       "1  3.840932e-10   1  1.552685e-99   1.0  3.280995e-09  4.502352e-271   0.0   \n",
       "2  3.840932e-10   1  1.552685e-99   1.0  3.280995e-09  4.502352e-271   0.0   \n",
       "3  3.840932e-10   1  1.552685e-99   1.0  3.280995e-09  4.502352e-271   0.0   \n",
       "4  3.840932e-10   1  1.552685e-99   1.0  3.280995e-09  4.502352e-271   0.0   \n",
       "\n",
       "           cl_5  \n",
       "0  3.840932e-10  \n",
       "1  3.840932e-10  \n",
       "2  3.840932e-10  \n",
       "3  3.840932e-10  \n",
       "4  3.840932e-10  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors = ['f{}'.format(i) for i in range(1,58)]\n",
    "for n in np.unique(df['cl']):\n",
    "        \n",
    "    df['S_cl_%s'%n] = np.dot(df[factors].values ,cl_logit[n].coef_[0].reshape(-1,1))\n",
    "    df['p_cl__%s'%n] = df['S_cl_%s'%n].groupby(by = df.event_id).apply(softmax)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>f32</th>\n",
       "      <th>f33</th>\n",
       "      <th>f34</th>\n",
       "      <th>f35</th>\n",
       "      <th>f36</th>\n",
       "      <th>f37</th>\n",
       "      <th>f38</th>\n",
       "      <th>f39</th>\n",
       "      <th>f40</th>\n",
       "      <th>f41</th>\n",
       "      <th>f42</th>\n",
       "      <th>f43</th>\n",
       "      <th>f44</th>\n",
       "      <th>f45</th>\n",
       "      <th>f46</th>\n",
       "      <th>f47</th>\n",
       "      <th>f48</th>\n",
       "      <th>f49</th>\n",
       "      <th>f50</th>\n",
       "      <th>f51</th>\n",
       "      <th>f52</th>\n",
       "      <th>f53</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>runner_id</th>\n",
       "      <th>result</th>\n",
       "      <th>event_id</th>\n",
       "      <th>is1</th>\n",
       "      <th>is2</th>\n",
       "      <th>oos</th>\n",
       "      <th>S</th>\n",
       "      <th>p</th>\n",
       "      <th>log_p</th>\n",
       "      <th>cl_x</th>\n",
       "      <th>cl_0_x</th>\n",
       "      <th>cl_1_x</th>\n",
       "      <th>cl_2_x</th>\n",
       "      <th>cl_3_x</th>\n",
       "      <th>cl_4_x</th>\n",
       "      <th>cl_5_x</th>\n",
       "      <th>S_cl_0</th>\n",
       "      <th>p_cl__0</th>\n",
       "      <th>S_cl_1</th>\n",
       "      <th>p_cl__1</th>\n",
       "      <th>S_cl_2</th>\n",
       "      <th>p_cl__2</th>\n",
       "      <th>S_cl_3</th>\n",
       "      <th>p_cl__3</th>\n",
       "      <th>S_cl_4</th>\n",
       "      <th>p_cl__4</th>\n",
       "      <th>S_cl_5</th>\n",
       "      <th>p_cl__5</th>\n",
       "      <th>p_mix</th>\n",
       "      <th>cl_y</th>\n",
       "      <th>cl_0_y</th>\n",
       "      <th>cl_1_y</th>\n",
       "      <th>cl_2_y</th>\n",
       "      <th>cl_3_y</th>\n",
       "      <th>cl_4_y</th>\n",
       "      <th>cl_5_y</th>\n",
       "      <th>cl</th>\n",
       "      <th>cl_0</th>\n",
       "      <th>cl_1</th>\n",
       "      <th>cl_2</th>\n",
       "      <th>cl_3</th>\n",
       "      <th>cl_4</th>\n",
       "      <th>cl_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174298</th>\n",
       "      <td>-0.321080</td>\n",
       "      <td>0.263185</td>\n",
       "      <td>0.116496</td>\n",
       "      <td>0.013721</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.098474</td>\n",
       "      <td>0.153590</td>\n",
       "      <td>-0.035127</td>\n",
       "      <td>0.136799</td>\n",
       "      <td>-0.028383</td>\n",
       "      <td>0.400107</td>\n",
       "      <td>0.116757</td>\n",
       "      <td>-0.209353</td>\n",
       "      <td>-0.064880</td>\n",
       "      <td>-0.085903</td>\n",
       "      <td>0.019455</td>\n",
       "      <td>-0.034992</td>\n",
       "      <td>-0.062260</td>\n",
       "      <td>-0.022169</td>\n",
       "      <td>0.028282</td>\n",
       "      <td>-0.050458</td>\n",
       "      <td>-0.183807</td>\n",
       "      <td>-0.217550</td>\n",
       "      <td>-0.143204</td>\n",
       "      <td>-0.010932</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>-0.335943</td>\n",
       "      <td>-0.007364</td>\n",
       "      <td>-0.101062</td>\n",
       "      <td>-0.014607</td>\n",
       "      <td>-0.570272</td>\n",
       "      <td>-0.123502</td>\n",
       "      <td>-0.648533</td>\n",
       "      <td>-0.002281</td>\n",
       "      <td>-0.035335</td>\n",
       "      <td>0.046689</td>\n",
       "      <td>0.085554</td>\n",
       "      <td>0.022733</td>\n",
       "      <td>-0.035128</td>\n",
       "      <td>0.215129</td>\n",
       "      <td>-0.502835</td>\n",
       "      <td>0.681566</td>\n",
       "      <td>0.045806</td>\n",
       "      <td>-0.051037</td>\n",
       "      <td>0.056224</td>\n",
       "      <td>-0.034666</td>\n",
       "      <td>-0.132721</td>\n",
       "      <td>0.051436</td>\n",
       "      <td>-0.354936</td>\n",
       "      <td>0.436300</td>\n",
       "      <td>-0.004281</td>\n",
       "      <td>-0.107041</td>\n",
       "      <td>0.359159</td>\n",
       "      <td>-0.051952</td>\n",
       "      <td>-0.035207</td>\n",
       "      <td>-0.035137</td>\n",
       "      <td>-0.032829</td>\n",
       "      <td>448142</td>\n",
       "      <td>2</td>\n",
       "      <td>337025</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.021203</td>\n",
       "      <td>0.194703</td>\n",
       "      <td>-1.930212</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999537</td>\n",
       "      <td>6.884783e-07</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.291616</td>\n",
       "      <td>0.136682</td>\n",
       "      <td>0.321453</td>\n",
       "      <td>0.266420</td>\n",
       "      <td>0.018743</td>\n",
       "      <td>0.192890</td>\n",
       "      <td>0.013695</td>\n",
       "      <td>0.186395</td>\n",
       "      <td>-0.044865</td>\n",
       "      <td>0.186884</td>\n",
       "      <td>-0.236799</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.136683</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174299</th>\n",
       "      <td>-0.014688</td>\n",
       "      <td>0.361564</td>\n",
       "      <td>-0.151997</td>\n",
       "      <td>-0.008917</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.128687</td>\n",
       "      <td>0.047174</td>\n",
       "      <td>0.016257</td>\n",
       "      <td>0.208812</td>\n",
       "      <td>0.229246</td>\n",
       "      <td>0.154541</td>\n",
       "      <td>0.101702</td>\n",
       "      <td>0.111508</td>\n",
       "      <td>0.026485</td>\n",
       "      <td>0.820248</td>\n",
       "      <td>-0.012151</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.033626</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>-0.009580</td>\n",
       "      <td>-0.022102</td>\n",
       "      <td>0.398254</td>\n",
       "      <td>-0.124395</td>\n",
       "      <td>0.020805</td>\n",
       "      <td>-0.010932</td>\n",
       "      <td>-0.001166</td>\n",
       "      <td>0.159201</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.015720</td>\n",
       "      <td>-0.020251</td>\n",
       "      <td>-0.186409</td>\n",
       "      <td>-0.063011</td>\n",
       "      <td>-0.119374</td>\n",
       "      <td>-0.002281</td>\n",
       "      <td>0.016662</td>\n",
       "      <td>-0.164145</td>\n",
       "      <td>-0.224057</td>\n",
       "      <td>0.784832</td>\n",
       "      <td>0.016252</td>\n",
       "      <td>0.197853</td>\n",
       "      <td>0.036053</td>\n",
       "      <td>-0.687208</td>\n",
       "      <td>0.145379</td>\n",
       "      <td>0.045346</td>\n",
       "      <td>-0.024538</td>\n",
       "      <td>0.070196</td>\n",
       "      <td>-0.132721</td>\n",
       "      <td>-0.063144</td>\n",
       "      <td>0.454167</td>\n",
       "      <td>0.532934</td>\n",
       "      <td>-0.004281</td>\n",
       "      <td>-0.298923</td>\n",
       "      <td>0.352397</td>\n",
       "      <td>-0.012849</td>\n",
       "      <td>0.016282</td>\n",
       "      <td>0.016245</td>\n",
       "      <td>-0.009948</td>\n",
       "      <td>492119</td>\n",
       "      <td>3</td>\n",
       "      <td>337025</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.054315</td>\n",
       "      <td>0.188361</td>\n",
       "      <td>-1.783385</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999537</td>\n",
       "      <td>6.884783e-07</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.252755</td>\n",
       "      <td>0.142099</td>\n",
       "      <td>-0.306361</td>\n",
       "      <td>0.142204</td>\n",
       "      <td>0.059433</td>\n",
       "      <td>0.200900</td>\n",
       "      <td>-0.621389</td>\n",
       "      <td>0.098769</td>\n",
       "      <td>0.303505</td>\n",
       "      <td>0.264769</td>\n",
       "      <td>-0.036182</td>\n",
       "      <td>0.190656</td>\n",
       "      <td>0.142099</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174300</th>\n",
       "      <td>0.365143</td>\n",
       "      <td>0.361564</td>\n",
       "      <td>-0.086240</td>\n",
       "      <td>0.006008</td>\n",
       "      <td>-0.007694</td>\n",
       "      <td>-0.039630</td>\n",
       "      <td>0.047174</td>\n",
       "      <td>0.011645</td>\n",
       "      <td>0.095136</td>\n",
       "      <td>0.199507</td>\n",
       "      <td>0.085536</td>\n",
       "      <td>-0.174135</td>\n",
       "      <td>0.125581</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.211824</td>\n",
       "      <td>0.029606</td>\n",
       "      <td>0.010887</td>\n",
       "      <td>0.025491</td>\n",
       "      <td>0.007185</td>\n",
       "      <td>-0.005459</td>\n",
       "      <td>0.076477</td>\n",
       "      <td>0.301935</td>\n",
       "      <td>0.190694</td>\n",
       "      <td>0.054606</td>\n",
       "      <td>0.089622</td>\n",
       "      <td>-0.000850</td>\n",
       "      <td>0.274039</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.084583</td>\n",
       "      <td>0.139013</td>\n",
       "      <td>-0.181795</td>\n",
       "      <td>-0.044473</td>\n",
       "      <td>0.058390</td>\n",
       "      <td>0.028426</td>\n",
       "      <td>0.012864</td>\n",
       "      <td>-0.093669</td>\n",
       "      <td>-0.141739</td>\n",
       "      <td>0.140447</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>-0.025098</td>\n",
       "      <td>0.807525</td>\n",
       "      <td>0.082426</td>\n",
       "      <td>-0.079214</td>\n",
       "      <td>0.030437</td>\n",
       "      <td>-0.002493</td>\n",
       "      <td>-0.024679</td>\n",
       "      <td>-0.132721</td>\n",
       "      <td>0.091730</td>\n",
       "      <td>0.370001</td>\n",
       "      <td>0.513737</td>\n",
       "      <td>0.029828</td>\n",
       "      <td>-0.185056</td>\n",
       "      <td>-0.036075</td>\n",
       "      <td>0.032794</td>\n",
       "      <td>0.013120</td>\n",
       "      <td>0.011713</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>493020</td>\n",
       "      <td>1</td>\n",
       "      <td>337025</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.019709</td>\n",
       "      <td>0.194994</td>\n",
       "      <td>-1.686988</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999537</td>\n",
       "      <td>6.884783e-07</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.125493</td>\n",
       "      <td>0.161383</td>\n",
       "      <td>-0.161143</td>\n",
       "      <td>0.164429</td>\n",
       "      <td>0.004230</td>\n",
       "      <td>0.190111</td>\n",
       "      <td>-0.291874</td>\n",
       "      <td>0.137318</td>\n",
       "      <td>-0.078714</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>-0.056763</td>\n",
       "      <td>0.186773</td>\n",
       "      <td>0.161384</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174301</th>\n",
       "      <td>-0.014688</td>\n",
       "      <td>0.227617</td>\n",
       "      <td>0.019758</td>\n",
       "      <td>-0.008917</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>-0.074838</td>\n",
       "      <td>-0.123969</td>\n",
       "      <td>0.016257</td>\n",
       "      <td>-0.075395</td>\n",
       "      <td>-0.136683</td>\n",
       "      <td>-0.436639</td>\n",
       "      <td>-0.058350</td>\n",
       "      <td>0.042018</td>\n",
       "      <td>0.026485</td>\n",
       "      <td>-0.040536</td>\n",
       "      <td>-0.054279</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>-0.004008</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>-0.009580</td>\n",
       "      <td>0.047789</td>\n",
       "      <td>0.138720</td>\n",
       "      <td>0.876229</td>\n",
       "      <td>0.013187</td>\n",
       "      <td>-0.010932</td>\n",
       "      <td>-0.002542</td>\n",
       "      <td>0.161147</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>0.015720</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>0.708288</td>\n",
       "      <td>0.325818</td>\n",
       "      <td>0.425668</td>\n",
       "      <td>-0.002281</td>\n",
       "      <td>0.016662</td>\n",
       "      <td>0.561480</td>\n",
       "      <td>0.210187</td>\n",
       "      <td>-0.002552</td>\n",
       "      <td>0.016252</td>\n",
       "      <td>-0.059128</td>\n",
       "      <td>-0.051544</td>\n",
       "      <td>-0.159210</td>\n",
       "      <td>0.022945</td>\n",
       "      <td>0.021892</td>\n",
       "      <td>-0.024538</td>\n",
       "      <td>0.027447</td>\n",
       "      <td>0.530884</td>\n",
       "      <td>-0.063144</td>\n",
       "      <td>-0.168643</td>\n",
       "      <td>-0.568762</td>\n",
       "      <td>-0.004281</td>\n",
       "      <td>0.641797</td>\n",
       "      <td>-0.621707</td>\n",
       "      <td>0.006831</td>\n",
       "      <td>0.016282</td>\n",
       "      <td>0.016245</td>\n",
       "      <td>0.011049</td>\n",
       "      <td>492127</td>\n",
       "      <td>4</td>\n",
       "      <td>337025</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.103867</td>\n",
       "      <td>0.179255</td>\n",
       "      <td>-1.822821</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999537</td>\n",
       "      <td>6.884783e-07</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.096432</td>\n",
       "      <td>0.166142</td>\n",
       "      <td>-0.159816</td>\n",
       "      <td>0.164647</td>\n",
       "      <td>-0.573824</td>\n",
       "      <td>0.106650</td>\n",
       "      <td>0.392566</td>\n",
       "      <td>0.272255</td>\n",
       "      <td>0.150692</td>\n",
       "      <td>0.227249</td>\n",
       "      <td>0.203261</td>\n",
       "      <td>0.242237</td>\n",
       "      <td>0.166142</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174302</th>\n",
       "      <td>-0.014688</td>\n",
       "      <td>-1.213931</td>\n",
       "      <td>0.101984</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>-0.112692</td>\n",
       "      <td>-0.123969</td>\n",
       "      <td>-0.009032</td>\n",
       "      <td>-0.365352</td>\n",
       "      <td>-0.263688</td>\n",
       "      <td>-0.203546</td>\n",
       "      <td>0.014026</td>\n",
       "      <td>-0.069754</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>-0.905632</td>\n",
       "      <td>0.017369</td>\n",
       "      <td>-0.008694</td>\n",
       "      <td>0.007151</td>\n",
       "      <td>-0.022672</td>\n",
       "      <td>-0.003663</td>\n",
       "      <td>-0.051706</td>\n",
       "      <td>-0.655102</td>\n",
       "      <td>-0.724978</td>\n",
       "      <td>0.054606</td>\n",
       "      <td>-0.056826</td>\n",
       "      <td>0.003015</td>\n",
       "      <td>-0.258445</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>-0.014961</td>\n",
       "      <td>-0.106823</td>\n",
       "      <td>0.230190</td>\n",
       "      <td>-0.094831</td>\n",
       "      <td>0.283849</td>\n",
       "      <td>-0.021582</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.350355</td>\n",
       "      <td>0.070055</td>\n",
       "      <td>-0.945461</td>\n",
       "      <td>-0.009095</td>\n",
       "      <td>-0.328756</td>\n",
       "      <td>-0.289198</td>\n",
       "      <td>0.082426</td>\n",
       "      <td>-0.134916</td>\n",
       "      <td>-0.046637</td>\n",
       "      <td>-0.004655</td>\n",
       "      <td>-0.038298</td>\n",
       "      <td>-0.132721</td>\n",
       "      <td>-0.016878</td>\n",
       "      <td>-0.300588</td>\n",
       "      <td>-0.914209</td>\n",
       "      <td>-0.016984</td>\n",
       "      <td>-0.050777</td>\n",
       "      <td>-0.053774</td>\n",
       "      <td>0.025176</td>\n",
       "      <td>-0.010478</td>\n",
       "      <td>-0.009067</td>\n",
       "      <td>0.029583</td>\n",
       "      <td>493021</td>\n",
       "      <td>5</td>\n",
       "      <td>337025</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.199094</td>\n",
       "      <td>0.242687</td>\n",
       "      <td>-1.078313</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999537</td>\n",
       "      <td>6.884783e-07</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.766296</td>\n",
       "      <td>0.393693</td>\n",
       "      <td>0.305866</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.491418</td>\n",
       "      <td>0.309449</td>\n",
       "      <td>0.507002</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>-0.330619</td>\n",
       "      <td>0.140434</td>\n",
       "      <td>0.126482</td>\n",
       "      <td>0.224334</td>\n",
       "      <td>0.393692</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1        f2        f3        f4        f5        f6        f7  \\\n",
       "174298 -0.321080  0.263185  0.116496  0.013721  0.001686  0.098474  0.153590   \n",
       "174299 -0.014688  0.361564 -0.151997 -0.008917  0.001686  0.128687  0.047174   \n",
       "174300  0.365143  0.361564 -0.086240  0.006008 -0.007694 -0.039630  0.047174   \n",
       "174301 -0.014688  0.227617  0.019758 -0.008917  0.001686 -0.074838 -0.123969   \n",
       "174302 -0.014688 -1.213931  0.101984 -0.001895  0.002635 -0.112692 -0.123969   \n",
       "\n",
       "              f8        f9       f10       f11       f12       f13       f14  \\\n",
       "174298 -0.035127  0.136799 -0.028383  0.400107  0.116757 -0.209353 -0.064880   \n",
       "174299  0.016257  0.208812  0.229246  0.154541  0.101702  0.111508  0.026485   \n",
       "174300  0.011645  0.095136  0.199507  0.085536 -0.174135  0.125581  0.008427   \n",
       "174301  0.016257 -0.075395 -0.136683 -0.436639 -0.058350  0.042018  0.026485   \n",
       "174302 -0.009032 -0.365352 -0.263688 -0.203546  0.014026 -0.069754  0.003484   \n",
       "\n",
       "             f15       f16       f17       f18       f19       f20       f21  \\\n",
       "174298 -0.085903  0.019455 -0.034992 -0.062260 -0.022169  0.028282 -0.050458   \n",
       "174299  0.820248 -0.012151  0.016400  0.033626  0.018828 -0.009580 -0.022102   \n",
       "174300  0.211824  0.029606  0.010887  0.025491  0.007185 -0.005459  0.076477   \n",
       "174301 -0.040536 -0.054279  0.016400 -0.004008  0.018828 -0.009580  0.047789   \n",
       "174302 -0.905632  0.017369 -0.008694  0.007151 -0.022672 -0.003663 -0.051706   \n",
       "\n",
       "             f22       f23       f24       f25       f26       f27       f28  \\\n",
       "174298 -0.183807 -0.217550 -0.143204 -0.010932  0.001542 -0.335943 -0.007364   \n",
       "174299  0.398254 -0.124395  0.020805 -0.010932 -0.001166  0.159201  0.000145   \n",
       "174300  0.301935  0.190694  0.054606  0.089622 -0.000850  0.274039  0.001102   \n",
       "174301  0.138720  0.876229  0.013187 -0.010932 -0.002542  0.161147  0.002912   \n",
       "174302 -0.655102 -0.724978  0.054606 -0.056826  0.003015 -0.258445  0.003205   \n",
       "\n",
       "             f29       f30       f31       f32       f33       f34       f35  \\\n",
       "174298 -0.101062 -0.014607 -0.570272 -0.123502 -0.648533 -0.002281 -0.035335   \n",
       "174299  0.015720 -0.020251 -0.186409 -0.063011 -0.119374 -0.002281  0.016662   \n",
       "174300  0.084583  0.139013 -0.181795 -0.044473  0.058390  0.028426  0.012864   \n",
       "174301  0.015720  0.002668  0.708288  0.325818  0.425668 -0.002281  0.016662   \n",
       "174302 -0.014961 -0.106823  0.230190 -0.094831  0.283849 -0.021582 -0.010854   \n",
       "\n",
       "             f36       f37       f38       f39       f40       f41       f42  \\\n",
       "174298  0.046689  0.085554  0.022733 -0.035128  0.215129 -0.502835  0.681566   \n",
       "174299 -0.164145 -0.224057  0.784832  0.016252  0.197853  0.036053 -0.687208   \n",
       "174300 -0.093669 -0.141739  0.140447  0.011719 -0.025098  0.807525  0.082426   \n",
       "174301  0.561480  0.210187 -0.002552  0.016252 -0.059128 -0.051544 -0.159210   \n",
       "174302 -0.350355  0.070055 -0.945461 -0.009095 -0.328756 -0.289198  0.082426   \n",
       "\n",
       "             f43       f44       f45       f46       f47       f48       f49  \\\n",
       "174298  0.045806 -0.051037  0.056224 -0.034666 -0.132721  0.051436 -0.354936   \n",
       "174299  0.145379  0.045346 -0.024538  0.070196 -0.132721 -0.063144  0.454167   \n",
       "174300 -0.079214  0.030437 -0.002493 -0.024679 -0.132721  0.091730  0.370001   \n",
       "174301  0.022945  0.021892 -0.024538  0.027447  0.530884 -0.063144 -0.168643   \n",
       "174302 -0.134916 -0.046637 -0.004655 -0.038298 -0.132721 -0.016878 -0.300588   \n",
       "\n",
       "             f50       f51       f52       f53       f54       f55       f56  \\\n",
       "174298  0.436300 -0.004281 -0.107041  0.359159 -0.051952 -0.035207 -0.035137   \n",
       "174299  0.532934 -0.004281 -0.298923  0.352397 -0.012849  0.016282  0.016245   \n",
       "174300  0.513737  0.029828 -0.185056 -0.036075  0.032794  0.013120  0.011713   \n",
       "174301 -0.568762 -0.004281  0.641797 -0.621707  0.006831  0.016282  0.016245   \n",
       "174302 -0.914209 -0.016984 -0.050777 -0.053774  0.025176 -0.010478 -0.009067   \n",
       "\n",
       "             f57  runner_id  result  event_id    is1    is2   oos         S  \\\n",
       "174298 -0.032829     448142       2    337025  False  False  True -0.021203   \n",
       "174299 -0.009948     492119       3    337025  False  False  True -0.054315   \n",
       "174300  0.002146     493020       1    337025  False  False  True -0.019709   \n",
       "174301  0.011049     492127       4    337025  False  False  True -0.103867   \n",
       "174302  0.029583     493021       5    337025  False  False  True  0.199094   \n",
       "\n",
       "               p     log_p  cl_x    cl_0_x        cl_1_x    cl_2_x  cl_3_x  \\\n",
       "174298  0.194703 -1.930212     0  0.999537  6.884783e-07  0.000459     0.0   \n",
       "174299  0.188361 -1.783385     0  0.999537  6.884783e-07  0.000459     0.0   \n",
       "174300  0.194994 -1.686988     0  0.999537  6.884783e-07  0.000459     0.0   \n",
       "174301  0.179255 -1.822821     0  0.999537  6.884783e-07  0.000459     0.0   \n",
       "174302  0.242687 -1.078313     0  0.999537  6.884783e-07  0.000459     0.0   \n",
       "\n",
       "        cl_4_x    cl_5_x    S_cl_0   p_cl__0    S_cl_1   p_cl__1    S_cl_2  \\\n",
       "174298     0.0  0.000003 -0.291616  0.136682  0.321453  0.266420  0.018743   \n",
       "174299     0.0  0.000003 -0.252755  0.142099 -0.306361  0.142204  0.059433   \n",
       "174300     0.0  0.000003 -0.125493  0.161383 -0.161143  0.164429  0.004230   \n",
       "174301     0.0  0.000003 -0.096432  0.166142 -0.159816  0.164647 -0.573824   \n",
       "174302     0.0  0.000003  0.766296  0.393693  0.305866  0.262300  0.491418   \n",
       "\n",
       "         p_cl__2    S_cl_3   p_cl__3    S_cl_4   p_cl__4    S_cl_5   p_cl__5  \\\n",
       "174298  0.192890  0.013695  0.186395 -0.044865  0.186884 -0.236799  0.156000   \n",
       "174299  0.200900 -0.621389  0.098769  0.303505  0.264769 -0.036182  0.190656   \n",
       "174300  0.190111 -0.291874  0.137318 -0.078714  0.180664 -0.056763  0.186773   \n",
       "174301  0.106650  0.392566  0.272255  0.150692  0.227249  0.203261  0.242237   \n",
       "174302  0.309449  0.507002  0.305263 -0.330619  0.140434  0.126482  0.224334   \n",
       "\n",
       "           p_mix  cl_y    cl_0_y    cl_1_y    cl_2_y  cl_3_y  cl_4_y  \\\n",
       "174298  0.136683     0  0.999993  0.000001  0.000003     0.0     0.0   \n",
       "174299  0.142099     0  0.999993  0.000001  0.000003     0.0     0.0   \n",
       "174300  0.161384     0  0.999993  0.000001  0.000003     0.0     0.0   \n",
       "174301  0.166142     0  0.999993  0.000001  0.000003     0.0     0.0   \n",
       "174302  0.393692     0  0.999993  0.000001  0.000003     0.0     0.0   \n",
       "\n",
       "          cl_5_y  cl      cl_0      cl_1      cl_2  cl_3  cl_4      cl_5  \n",
       "174298  0.000003   0  0.999993  0.000001  0.000003   0.0   0.0  0.000003  \n",
       "174299  0.000003   0  0.999993  0.000001  0.000003   0.0   0.0  0.000003  \n",
       "174300  0.000003   0  0.999993  0.000001  0.000003   0.0   0.0  0.000003  \n",
       "174301  0.000003   0  0.999993  0.000001  0.000003   0.0   0.0  0.000003  \n",
       "174302  0.000003   0  0.999993  0.000001  0.000003   0.0   0.0  0.000003  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_mix = np.zeros((len(df)))\n",
    "df.fillna(0, inplace =True)\n",
    "for n in np.unique(df['cl']):\n",
    "    \n",
    "    prob_mix = prob_mix + df['cl_%s'%n]*df['p_cl__%s'%n]\n",
    "df['p_mix'] = prob_mix\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df.groupby(df.event_id).apply(lambda x: x.result[x.p_mix.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2589"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df_result ==1)|(df_result ==2)|(df_result ==3)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll is1 -2769.0101766\n",
      "ll oos -2759.33303157\n"
     ]
    }
   ],
   "source": [
    "df['log_p'] = np.log(df['p_mix'])\n",
    "print 'll is1',df.ix[df.is1 & (df.result ==1),'log_p'].mean()*1000\n",
    "print 'll oos',df.ix[df.oos & (df.result ==1),'log_p'].mean()*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster  0,  count 3732\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "train score   0.60691318328\n",
      "\n",
      "cluster  1,  count 5563\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "train score   0.596620528492\n",
      "\n",
      "cluster  2,  count 1426\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "train score   0.672510518934\n",
      "\n",
      "cluster  3,  count 20\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "train score   1.0\n",
      "\n",
      "cluster  4,  count 11\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "train score   1.0\n",
      "\n",
      "cluster  5,  count 1637\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "train score   0.64447159438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components =2)\n",
    "cl_pca =[]\n",
    "cl_logit_pca = []\n",
    "for n in np.unique(df_cluster['cl']):\n",
    "    \n",
    "    logit1 = LogisticRegression (C =1., penalty ='l2', n_jobs =-1, class_weight ='balanced')\n",
    "    X_tr_cl = X_train[(df_cluster['cl'] == n).values]\n",
    "    y_tr_cl = y_train[(df_cluster['cl'] == n).values]\n",
    "    X_new, pca = PCA_features(X_tr_cl, pca)\n",
    "    \n",
    "    print 'cluster  %s,  count %s'%(n,len(y_tr_cl))\n",
    "    print logit1.fit(X_new, y_tr_cl)\n",
    "    print 'train score  ',logit1.score(X_new, y_tr_cl)\n",
    "    print\n",
    "    cl_pca.append(pca)\n",
    "    cl_logit_pca.append(logit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -1 clustr   max diff 133 coef 1.51116303658\n",
      "0 -2 clustr   max diff 82 coef 1.28600879329\n",
      "0 -5 clustr   max diff 118 coef 0.969379582651\n",
      "1 -2 clustr   max diff 131 coef 1.26791787489\n",
      "1 -5 clustr   max diff 2 coef 1.74322336122\n",
      "2 -5 clustr   max diff 13 coef 1.43939797241\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "for n,k in combinations([0, 1, 2, 5], 2):\n",
    "    diff_coef = np.abs(cl_logit_pca[n].coef_[0] -cl_logit_pca[k].coef_[0])\n",
    "    print '{} -{} clustr   max diff {} coef {}'.format(n,k,diff_coef.argmax(), diff_coef.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_y_data, df_event = ranking_data(df[df.is1], [2])\n",
    "X_train, y_train = X_y_data[:,:-1], X_y_data[:,-1]\n",
    "X_y_data, df_event_ts = ranking_data(df[df.oos], [2])\n",
    "X_test, y_test = X_y_data[:,:-1], X_y_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures()\n",
    "X_poly_train = poly.fit_transform(X_train)\n",
    "X_poly_test = poly.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12389L, 1711L), (4555L, 1711L))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly_train.shape, X_poly_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_p = logit1 = LogisticRegression (C =1., penalty ='l2', n_jobs =-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_p.fit(X_poly_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64799418839292922"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_p.score(X_poly_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54753018660812292"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, logit_p.predict(X_poly_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time GBC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57252415013642066"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_acc = accuracy_score(y_test, GBC.predict(X_test))\n",
    "gbc_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13561L,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBC.decision_function(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll is1 -2529.42053198\n",
      "ll oos -2572.45382418\n"
     ]
    }
   ],
   "source": [
    "df['S_gbc'] = GBC.decision_function(df[factors].values)\n",
    "df['p_gbc'] = df['S_gbc'].groupby(by = df.event_id).apply(softmax)\n",
    "df['log_p_gbc'] = np.log(df['p_gbc'])\n",
    "print 'll is1',df.ix[df.is1 & (df.result ==1),'log_p_gbc'].mean()*1000\n",
    "print 'll oos',df.ix[df.oos & (df.result ==1),'log_p_gbc'].mean()*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>oos</th>\n",
       "      <th>S</th>\n",
       "      <th>p</th>\n",
       "      <th>log_p</th>\n",
       "      <th>S_pca</th>\n",
       "      <th>p_pca</th>\n",
       "      <th>log_p_pca</th>\n",
       "      <th>S_gbc</th>\n",
       "      <th>p_gbc</th>\n",
       "      <th>log_p_gbc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.152068</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.971937</td>\n",
       "      <td>1.096287</td>\n",
       "      <td>-0.003309</td>\n",
       "      <td>1.761018</td>\n",
       "      <td>0.968297</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.563592</td>\n",
       "      <td>0.039677</td>\n",
       "      <td>-3.226976</td>\n",
       "      <td>-0.518235</td>\n",
       "      <td>0.041320</td>\n",
       "      <td>-3.186417</td>\n",
       "      <td>-0.634022</td>\n",
       "      <td>0.038235</td>\n",
       "      <td>-3.264015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.137530</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.398019</td>\n",
       "      <td>0.481980</td>\n",
       "      <td>-0.002772</td>\n",
       "      <td>1.682896</td>\n",
       "      <td>0.354919</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.298126</td>\n",
       "      <td>0.051741</td>\n",
       "      <td>-2.961511</td>\n",
       "      <td>-0.314351</td>\n",
       "      <td>0.050664</td>\n",
       "      <td>-2.982533</td>\n",
       "      <td>-0.367064</td>\n",
       "      <td>0.049934</td>\n",
       "      <td>-2.997057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.250759</td>\n",
       "      <td>1.029570</td>\n",
       "      <td>-0.145221</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.307072</td>\n",
       "      <td>0.427015</td>\n",
       "      <td>-0.012089</td>\n",
       "      <td>1.165470</td>\n",
       "      <td>0.440719</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.199542</td>\n",
       "      <td>0.057101</td>\n",
       "      <td>-2.862927</td>\n",
       "      <td>-0.149040</td>\n",
       "      <td>0.059772</td>\n",
       "      <td>-2.817222</td>\n",
       "      <td>-0.279891</td>\n",
       "      <td>0.054482</td>\n",
       "      <td>-2.909885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.395654</td>\n",
       "      <td>-0.136884</td>\n",
       "      <td>-0.020776</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.234280</td>\n",
       "      <td>0.361007</td>\n",
       "      <td>0.009897</td>\n",
       "      <td>0.880806</td>\n",
       "      <td>1.230350</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.397436</td>\n",
       "      <td>0.046849</td>\n",
       "      <td>-3.060820</td>\n",
       "      <td>-0.416026</td>\n",
       "      <td>0.045766</td>\n",
       "      <td>-3.084208</td>\n",
       "      <td>-0.418661</td>\n",
       "      <td>0.047423</td>\n",
       "      <td>-3.048654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064913</td>\n",
       "      <td>1.343377</td>\n",
       "      <td>0.242782</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.017472</td>\n",
       "      <td>-0.199782</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.481565</td>\n",
       "      <td>1.404777</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.321414</td>\n",
       "      <td>0.050550</td>\n",
       "      <td>-2.984798</td>\n",
       "      <td>-0.297887</td>\n",
       "      <td>0.051505</td>\n",
       "      <td>-2.966069</td>\n",
       "      <td>-0.490446</td>\n",
       "      <td>0.044138</td>\n",
       "      <td>-3.120440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2        f3        f4      f5        f6        f7  \\\n",
       "0  0.064913  1.395654 -0.152068  0.008172  0.0007  0.971937  1.096287   \n",
       "1 -0.250759  1.137530  0.019747  0.008894  0.0007  0.398019  0.481980   \n",
       "2 -0.250759  1.029570 -0.145221 -0.020776  0.0007  0.307072  0.427015   \n",
       "3  0.064913  1.395654 -0.136884 -0.020776  0.0007  0.234280  0.361007   \n",
       "4  0.064913  1.343377  0.242782  0.008894  0.0007 -0.017472 -0.199782   \n",
       "\n",
       "         f8        f9       f10    ...        oos         S         p  \\\n",
       "0 -0.003309  1.761018  0.968297    ...      False -0.563592  0.039677   \n",
       "1 -0.002772  1.682896  0.354919    ...      False -0.298126  0.051741   \n",
       "2 -0.012089  1.165470  0.440719    ...      False -0.199542  0.057101   \n",
       "3  0.009897  0.880806  1.230350    ...      False -0.397436  0.046849   \n",
       "4  0.004808  0.481565  1.404777    ...      False -0.321414  0.050550   \n",
       "\n",
       "      log_p     S_pca     p_pca  log_p_pca     S_gbc     p_gbc  log_p_gbc  \n",
       "0 -3.226976 -0.518235  0.041320  -3.186417 -0.634022  0.038235  -3.264015  \n",
       "1 -2.961511 -0.314351  0.050664  -2.982533 -0.367064  0.049934  -2.997057  \n",
       "2 -2.862927 -0.149040  0.059772  -2.817222 -0.279891  0.054482  -2.909885  \n",
       "3 -3.060820 -0.416026  0.045766  -3.084208 -0.418661  0.047423  -3.048654  \n",
       "4 -2.984798 -0.297887  0.051505  -2.966069 -0.490446  0.044138  -3.120440  \n",
       "\n",
       "[5 rows x 72 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "accuracy   0.573040336258\n",
      "\n",
      "is1  mean ll  -2372.27720991\n",
      "\n",
      "oos  mean ll   -2409.29576695\n"
     ]
    }
   ],
   "source": [
    "columns = ['f{}'.format(i) for i in range(1,58)]+ ['log_p_mean'] +['result','event_id']\n",
    "#columns\n",
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(C = 0.10 )\n",
    "clf_llhood(df, svm, [2, 3, 4], columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 45,  7, 34, 55, 50, 16, 54, 20, 48, 41, 19, 12, 24, 23,  4, 46,\n",
       "       32, 26, 40, 28, 18, 31,  2, 33, 36, 17,  9, 27, 51,  0, 13, 43,  5,\n",
       "       25, 11, 42, 52, 15, 35, 37, 47, 53, 14,  3, 30, 10, 56, 22, 21, 44,\n",
       "       29, 39,  8, 49,  6,  1], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(np.abs(GBC.feature_importances_), order =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00084886972704853166, 0.053257303242747057)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBC.feature_importances_[38], GBC.feature_importances_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 28,  1, 30, 56, 41, 42, 51,  5, 45, 21, 32,  9, 22, 39, 14, 23,\n",
       "        4, 50, 35, 11, 48, 10, 54, 38, 55,  7, 17, 16, 25, 26, 13, 40, 53,\n",
       "       20, 52, 31,  6, 18, 24,  8, 34, 15,  0, 49, 27, 37, 46, 29, 43, 47,\n",
       "       19,  3, 36,  2, 33, 44], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(np.abs(logit.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0016302758853066514, -0.37900786531156072, 0.28502828870319247)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.coef_[0][12], logit.coef_[0][44], logit.coef_[0][33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = np.hstack((X_train ,X_train* X_train[:,44].reshape(-1,1)))\n",
    "X_test_new = np.hstack((X_test, X_test* X_test[:,44].reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit2.fit(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56861588378438166"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, logit2.predict(X_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll is1 -2289.44713932\n",
      "ll oos -2321.91678668\n"
     ]
    }
   ],
   "source": [
    "df['S_sum'] = df['p'] +df['p_gbc']\n",
    "df['p_mean'] = df['S_sum'].groupby(by = df.event_id).apply(softmax)\n",
    "df['log_p_mean'] = np.log(df['p_mean'])\n",
    "print 'll is1',df.ix[df.is1 & (df.result ==1),'log_p_mean'].mean()*1000\n",
    "print 'll oos',df.ix[df.oos & (df.result ==1),'log_p_mean'].mean()*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVM = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time SVM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57215544576358679"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_acc = accuracy_score(y_test, SVM.predict(X_test))\n",
    "SVM_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ERT = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.83 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time ERT.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52909077501659174"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ERT_acc = accuracy_score(y_test, ERT.predict(X_test))\n",
    "ERT_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=100, random_state=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time abc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56596121229997787"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc_acc = accuracy_score(y_test, abc.predict(X_test))\n",
    "abc_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53063933338249392"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "clf_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 51.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(50, 20))\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52370769117321736"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "clf_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>runner_id_1</th>\n",
       "      <th>runner_id_2</th>\n",
       "      <th>runner_id_3</th>\n",
       "      <th>runner_id_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>287398</td>\n",
       "      <td>361432</td>\n",
       "      <td>355937</td>\n",
       "      <td>353046</td>\n",
       "      <td>363590.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>287399</td>\n",
       "      <td>349555</td>\n",
       "      <td>346146</td>\n",
       "      <td>358422</td>\n",
       "      <td>363600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287400</td>\n",
       "      <td>288738</td>\n",
       "      <td>145512</td>\n",
       "      <td>164898</td>\n",
       "      <td>165432.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>287401</td>\n",
       "      <td>347129</td>\n",
       "      <td>302596</td>\n",
       "      <td>277210</td>\n",
       "      <td>323071.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>287402</td>\n",
       "      <td>356489</td>\n",
       "      <td>363613</td>\n",
       "      <td>313967</td>\n",
       "      <td>343102.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_id  runner_id_1  runner_id_2  runner_id_3  runner_id_4\n",
       "0    287398       361432       355937       353046     363590.0\n",
       "1    287399       349555       346146       358422     363600.0\n",
       "2    287400       288738       145512       164898     165432.0\n",
       "3    287401       347129       302596       277210     323071.0\n",
       "4    287402       356489       363613       313967     343102.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n =2\n",
    "df_runner = df[['event_id', 'runner_id']][df['result'] == n-1]\n",
    "for n in [2, 3, 4]:\n",
    "    df_runner2 = df[['event_id', 'runner_id']][df['result'] == n]\n",
    "    df_runner = pd.merge(df_runner, df_runner2, how = 'left', left_on = 'event_id', right_on = 'event_id' ,\n",
    "                         suffixes =('_%s'%(n-1), '_%s'%n))\n",
    "df_runner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_parwise(X_matrix):\n",
    "    \"\"\"\n",
    "    return the matrix with data in each row is the differane between one choice row and other \n",
    "    <X_matrix> matrix where last column has runner_id and others columns have data\n",
    "    \"\"\"\n",
    "    if not type(X_matrix) is np.ndarray:\n",
    "        X_matrix = X_matrix.values\n",
    "    one = np.random.choice(X_matrix[:,-1])\n",
    "    mask_one = np.in1d(X_matrix[:,-1], one)\n",
    "    X_one = X_matrix[np.logical_not(mask_one)]\n",
    "    X_one[:,:-1] = X_one[:,:-1] - X_matrix[mask_one, :-1]\n",
    "    #print X_one.shape, np.repeat(one, len(X_one)).reshape(-1,1).shape\n",
    "    col_names = ['f{}'.format(i) for i in range(1,58)] + ['', 'runner_2', 'runner_1']\n",
    "    df_ = pd.DataFrame(data = np.hstack((X_one, np.repeat(one, len(X_one)).reshape(-1,1))), columns= col_names )\n",
    "    return df_.drop('', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = ['f{}'.format(i) for i in range(1,58)]\n",
    "df_parwise = df[factors +['event_id', 'runner_id']].groupby('event_id').apply(matrix_parwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f50</th>\n",
       "      <th>f51</th>\n",
       "      <th>f52</th>\n",
       "      <th>f53</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>runner_2</th>\n",
       "      <th>runner_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">287398</th>\n",
       "      <th>0</th>\n",
       "      <td>0.315672</td>\n",
       "      <td>0.366084</td>\n",
       "      <td>-0.006847</td>\n",
       "      <td>0.028947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664865</td>\n",
       "      <td>0.669272</td>\n",
       "      <td>0.008781</td>\n",
       "      <td>0.595547</td>\n",
       "      <td>0.527577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.028659</td>\n",
       "      <td>0.188608</td>\n",
       "      <td>-0.177171</td>\n",
       "      <td>0.008799</td>\n",
       "      <td>0.008780</td>\n",
       "      <td>0.209790</td>\n",
       "      <td>353046.0</td>\n",
       "      <td>342962.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107960</td>\n",
       "      <td>0.164968</td>\n",
       "      <td>0.029670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090947</td>\n",
       "      <td>0.054965</td>\n",
       "      <td>0.009317</td>\n",
       "      <td>0.517426</td>\n",
       "      <td>-0.085800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481149</td>\n",
       "      <td>-0.830927</td>\n",
       "      <td>-0.038244</td>\n",
       "      <td>0.009336</td>\n",
       "      <td>0.009317</td>\n",
       "      <td>0.386017</td>\n",
       "      <td>352217.0</td>\n",
       "      <td>342962.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.315672</td>\n",
       "      <td>0.366084</td>\n",
       "      <td>0.008337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.072792</td>\n",
       "      <td>-0.066008</td>\n",
       "      <td>0.021986</td>\n",
       "      <td>-0.284664</td>\n",
       "      <td>0.789631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169170</td>\n",
       "      <td>0.119652</td>\n",
       "      <td>0.181836</td>\n",
       "      <td>0.022032</td>\n",
       "      <td>0.021985</td>\n",
       "      <td>0.499794</td>\n",
       "      <td>361432.0</td>\n",
       "      <td>342962.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.315672</td>\n",
       "      <td>0.313807</td>\n",
       "      <td>0.388003</td>\n",
       "      <td>0.029670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.324543</td>\n",
       "      <td>-0.626797</td>\n",
       "      <td>0.016897</td>\n",
       "      <td>-0.683905</td>\n",
       "      <td>0.964058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.548292</td>\n",
       "      <td>-0.198511</td>\n",
       "      <td>-0.542613</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>0.016896</td>\n",
       "      <td>0.027179</td>\n",
       "      <td>359420.0</td>\n",
       "      <td>342962.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.433328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346631</td>\n",
       "      <td>0.024042</td>\n",
       "      <td>-0.002916</td>\n",
       "      <td>-0.283627</td>\n",
       "      <td>-0.652280</td>\n",
       "      <td>0.013406</td>\n",
       "      <td>-1.003390</td>\n",
       "      <td>-0.319131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137875</td>\n",
       "      <td>0.032719</td>\n",
       "      <td>0.834669</td>\n",
       "      <td>-0.774801</td>\n",
       "      <td>-0.193563</td>\n",
       "      <td>0.014596</td>\n",
       "      <td>0.013467</td>\n",
       "      <td>0.181092</td>\n",
       "      <td>363590.0</td>\n",
       "      <td>342962.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  f1        f2        f3        f4        f5        f6  \\\n",
       "event_id                                                                 \n",
       "287398   0  0.315672  0.366084 -0.006847  0.028947  0.000000  0.664865   \n",
       "         1  0.000000  0.107960  0.164968  0.029670  0.000000  0.090947   \n",
       "         2  0.315672  0.366084  0.008337  0.000000  0.000000 -0.072792   \n",
       "         3  0.315672  0.313807  0.388003  0.029670  0.000000 -0.324543   \n",
       "         4  0.433328  0.000000  0.346631  0.024042 -0.002916 -0.283627   \n",
       "\n",
       "                  f7        f8        f9       f10    ...          f50  \\\n",
       "event_id                                              ...                \n",
       "287398   0  0.669272  0.008781  0.595547  0.527577    ...     0.192288   \n",
       "         1  0.054965  0.009317  0.517426 -0.085800    ...    -0.049795   \n",
       "         2 -0.066008  0.021986 -0.284664  0.789631    ...     0.192288   \n",
       "         3 -0.626797  0.016897 -0.683905  0.964058    ...     0.631435   \n",
       "         4 -0.652280  0.013406 -1.003390 -0.319131    ...     0.137875   \n",
       "\n",
       "                 f51       f52       f53       f54       f55       f56  \\\n",
       "event_id                                                                 \n",
       "287398   0  0.000000 -0.028659  0.188608 -0.177171  0.008799  0.008780   \n",
       "         1  0.000000  0.481149 -0.830927 -0.038244  0.009336  0.009317   \n",
       "         2  0.000000  0.169170  0.119652  0.181836  0.022032  0.021985   \n",
       "         3  0.000000  0.548292 -0.198511 -0.542613  0.016932  0.016896   \n",
       "         4  0.032719  0.834669 -0.774801 -0.193563  0.014596  0.013467   \n",
       "\n",
       "                 f57  runner_2  runner_1  \n",
       "event_id                                  \n",
       "287398   0  0.209790  353046.0  342962.0  \n",
       "         1  0.386017  352217.0  342962.0  \n",
       "         2  0.499794  361432.0  342962.0  \n",
       "         3  0.027179  359420.0  342962.0  \n",
       "         4  0.181092  363590.0  342962.0  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parwise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
